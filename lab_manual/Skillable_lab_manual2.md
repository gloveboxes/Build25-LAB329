
# Step 2: Fine-tune and Optimize (15 min)

**Notebook:** `02.AzureML_FineTuningAndConvertByMSOlive.ipynb`

**Purpose:** Transform the small student model by fine-tuning it on the training data generated from the teacher model, and optimize it for deployment.

#### Instructions

1. **Open the notebook** from the file explorer in Azure ML Studio.

---

# Fine-Tuning and Model Conversion with Microsoft Olive

This notebook (`02.AzureML_FineTuningAndConvertByMSOlive.ipynb`) implements the second critical phase in our model distillation pipeline: fine-tuning the student model with the knowledge captured from the teacher model, and then optimizing it for efficient deployment.

## Workflow Overview

1. **Environment Setup**: Installation of essential libraries for model fine-tuning and optimization
2. **Fine-Tuning with LoRA**: Parameter-efficient tuning of Phi-4-mini-instruct using distilled knowledge
3. **Model Optimization**: Converting and optimizing the model to ONNX format with int4 quantization

## Technical Components

### Environment Setup
The notebook begins by installing several essential packages:
- PyTorch with CUDA 12.4 support for GPU acceleration
- Microsoft Olive AI toolkit for fine-tuning and optimization
- ONNX Runtime and GenAI extensions for efficient inference
- Supporting libraries like transformers, PEFT, and bitsandbytes

### Model Fine-Tuning
We use Microsoft's Olive fine-tuning capabilities with the LoRA (Low-Rank Adaptation) approach:

```bash
olive finetune \
    --method lora \
    --model_name_or_path azureml://registries/azureml/models/Phi-4-mini-instruct/versions/1 \
    --trust_remote_code \
    --data_name json \
    --data_files ./data/train_data.jsonl \
    --text_template "<|user|>{Question}<|end|><|assistant|>{Answer}<|end|>" \
    --max_steps 100 \
    --output_path models/phi-4-mini/ft \
    --target_modules "q_proj","k_proj","v_proj","o_proj","gate_proj","up_proj","down_proj" \
    --log_level 1
```

Key aspects:
- **LoRA Method**: Updates only a small subset of parameters, making fine-tuning more efficient
- **Base Model**: Uses Phi-4-mini-instruct from Azure ML model registry
- **Training Data**: Uses the JSONL file generated by the teacher model in the previous step
- **Chat Template**: Formats input/output in a consistent chat pattern
- **Target Modules**: Fine-tunes specific attention and MLP components of the transformer
- **Training Scope**: Limited to 100 steps to demonstrate the process while being computationally efficient

### Model Optimization
After fine-tuning, we optimize the model using Olive's auto-optimization capabilities:

```bash
olive auto-opt \
    --model_name_or_path azureml://registries/azureml/models/Phi-4-mini-instruct/versions/1 \
    --adapter_path models/phi-4-mini/ft/adapter \
    --device cpu \
    --provider CPUExecutionProvider \
    --use_model_builder \
    --precision int4 \
    --output_path models/phi-4-mini/onnx \
    --log_level 1
```

Key aspects:
- **Base Model + Adapter**: Combines the original Phi-4-mini model with our fine-tuned adapter
- **Deployment Target**: Optimized for CPU inference
- **Quantization**: Reduces precision to int4 (4-bit integers), dramatically reducing model size
- **Output Format**: ONNX, providing wide compatibility across various deployment environments

## Benefits of This Approach

1. **Efficiency**: LoRA fine-tuning is significantly faster and requires less computational resources than full model fine-tuning
2. **Knowledge Transfer**: Successfully transfers knowledge from the larger teacher model to the smaller student model
3. **Deployment Optimization**: Int4 quantization drastically reduces model size, enabling deployment on edge devices
4. **Performance**: Despite the optimizations, the model maintains most of its performance on the target task

## Next Steps

After completing this notebook:
1. The fine-tuned and optimized model is ready for inference (demonstrated in notebook 03)
2. The ONNX format enables efficient deployment across various platforms
3. The int4 quantization makes the model suitable for resource-constrained environments

This technique demonstrates how Microsoft's Olive toolkit simplifies the complex process of fine-tuning and optimizing transformer models while significantly improving their deployment characteristics.
