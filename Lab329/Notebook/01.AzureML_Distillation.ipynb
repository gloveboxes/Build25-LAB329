{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13490c0a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Ensure you login to Azure az login - device\n",
    "\n",
    "Ensure you change the Kernel to Python 3.10 AzureML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddced75",
   "metadata": {
    "gather": {
     "logged": 1745565408411
    }
   },
   "outputs": [],
   "source": [
    "pip install dotenv-azd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebb8ff3",
   "metadata": {
    "gather": {
     "logged": 1745565221871
    }
   },
   "outputs": [],
   "source": [
    "pip install datasets tqdm -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9605ccc",
   "metadata": {
    "gather": {
     "logged": 1745565223801
    }
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from abc import ABC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806f5e03",
   "metadata": {
    "gather": {
     "logged": 1745565224814
    }
   },
   "outputs": [],
   "source": [
    "class InputDataset(ABC):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        (\n",
    "            self.train_data_file_name,\n",
    "            self.test_data_file_name,\n",
    "            self.eval_data_file_name,\n",
    "        ) = (None, None, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34315645",
   "metadata": {
    "gather": {
     "logged": 1745565225277
    }
   },
   "outputs": [],
   "source": [
    "class CQnAHuggingFaceInputDataset(InputDataset):\n",
    "    \"\"\"\n",
    "    Loads the HuggingFace dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def load_hf_dataset(\n",
    "        self,\n",
    "        dataset_name,\n",
    "        train_sample_size=10,\n",
    "        val_sample_size=10,\n",
    "        test_sample_size=10,\n",
    "        train_split_name=\"train\",\n",
    "        val_split_name=\"validation\",\n",
    "        test_split_name=\"test\",\n",
    "    ):\n",
    "        full_dataset = load_dataset(dataset_name)\n",
    "\n",
    "        if val_split_name is not None:\n",
    "            train_data = full_dataset[train_split_name].select(range(train_sample_size))\n",
    "            val_data = full_dataset[val_split_name].select(range(val_sample_size))\n",
    "            test_data = full_dataset[test_split_name].select(range(test_sample_size))\n",
    "        else:\n",
    "            train_val_data = full_dataset[train_split_name].select(\n",
    "                range(train_sample_size + val_sample_size)\n",
    "            )\n",
    "            train_data = train_val_data.select(range(train_sample_size))\n",
    "            val_data = train_val_data.select(\n",
    "                range(train_sample_size, train_sample_size + val_sample_size)\n",
    "            )\n",
    "            test_data = full_dataset[test_split_name].select(range(test_sample_size))\n",
    "\n",
    "        return train_data, val_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a92e682",
   "metadata": {
    "gather": {
     "logged": 1745565225856
    }
   },
   "outputs": [],
   "source": [
    "# We can define train and test sample sizes here. Validation size is kept same as test sample size\n",
    "train_sample_size = 100\n",
    "val_sample_size = 100\n",
    "\n",
    "# Sample notebook using the dataset: https://huggingface.co/datasets/tau/commonsense_qa\n",
    "dataset_name = \"tau/commonsense_qa\"\n",
    "input_dataset = CQnAHuggingFaceInputDataset()\n",
    "\n",
    "# Note: train_split_name and test_split_name can vary by dataset. They are passed as arguments in load_hf_dataset.\n",
    "# If validation_split_name is None, the below function will split the train set to create the specified sized validation set.\n",
    "train, val, _ = input_dataset.load_hf_dataset(\n",
    "    dataset_name=dataset_name,\n",
    "    train_sample_size=train_sample_size,\n",
    "    val_sample_size=val_sample_size,\n",
    "    train_split_name=\"train\",\n",
    "    val_split_name=\"validation\",\n",
    ")\n",
    "\n",
    "print(\"Len of train data sample is \" + str(len(train)))\n",
    "print(\"Len of validation data sample is \" + str(len(val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a458770",
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir -p data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25914972",
   "metadata": {
    "gather": {
     "logged": 1745565227374
    }
   },
   "outputs": [],
   "source": [
    "train_data_path = \"./data/train_original_data.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1f3072",
   "metadata": {
    "gather": {
     "logged": 1745565227781
    }
   },
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad051ac1",
   "metadata": {
    "gather": {
     "logged": 1745565228450
    }
   },
   "outputs": [],
   "source": [
    "system_prompt = \"You are a helpful assistant. Your output should only be one of the five choices: 'A', 'B', 'C', 'D', or 'E'.\"\n",
    "user_prompt_template = \"Answer the following multiple-choice question by selecting the correct option.\\n\\nQuestion: {question}\\nAnswer Choices:\\n{answer_choices}\"\n",
    "\n",
    "# First, create/truncate the file to ensure it's empty and exists\n",
    "with open(train_data_path, \"w\", encoding='utf-8') as f:\n",
    "    pass  # Just create the file, we'll append to it later\n",
    "\n",
    "print(f\"Generating training data and writing to {train_data_path}\")\n",
    "\n",
    "# Check if train data has expected structure\n",
    "if len(train) == 0:\n",
    "    print(\"Error: Train dataset is empty!\")\n",
    "else:\n",
    "    print(f\"Sample train data keys: {list(train[0].keys())}\")\n",
    "    \n",
    "    # Verify train data has the expected fields before processing\n",
    "    sample = train[0]\n",
    "    if 'question' in sample and 'choices' in sample:\n",
    "        for i, row in enumerate(train):\n",
    "            try:\n",
    "                data = {\"messages\": []}\n",
    "                data[\"messages\"].append(\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": system_prompt,\n",
    "                    }\n",
    "                )\n",
    "                question, choices = row[\"question\"], row[\"choices\"]\n",
    "                labels, choice_list = choices[\"label\"], choices[\"text\"]\n",
    "                answer_choices = [\n",
    "                    \"({}) {}\".format(labels[i], choice_list[i]) for i in range(len(labels))\n",
    "                ]\n",
    "                answer_choices = \"\\n\".join(answer_choices)\n",
    "                data[\"messages\"].append(\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": user_prompt_template.format(\n",
    "                            question=question, answer_choices=answer_choices\n",
    "                        ),\n",
    "                    }\n",
    "                )\n",
    "                \n",
    "                # Write data as valid JSON line\n",
    "                with open(train_data_path, \"a\", encoding='utf-8') as f:\n",
    "                    f.write(json.dumps(data, ensure_ascii=False) + \"\\n\")\n",
    "                \n",
    "                if i % 10 == 0:  # Show progress every 10 items\n",
    "                    print(f\"Processed {i+1}/{len(train)} questions\", end=\"\\r\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"\\nError processing row {i}: {str(e)}\")\n",
    "                print(f\"Row data: {row}\")\n",
    "        \n",
    "        print(f\"\\nSuccessfully wrote {len(train)} questions to {train_data_path}\")\n",
    "        \n",
    "        # Validate the file has content\n",
    "        with open(train_data_path, \"r\", encoding='utf-8') as f:\n",
    "            first_line = f.readline().strip()\n",
    "            print(f\"File validation: {'SUCCESS' if first_line else 'FAILED - Empty file'}\")\n",
    "            if first_line:\n",
    "                # Try parsing the first line to validate JSON\n",
    "                try:\n",
    "                    json_data = json.loads(first_line)\n",
    "                    print(f\"JSON validation: SUCCESS\")\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"JSON validation: FAILED - {str(e)}\")\n",
    "    else:\n",
    "        print(f\"Error: Train data doesn't have the expected structure. Found keys: {list(sample.keys())}\")\n",
    "        print(\"Expected 'question' and 'choices' keys in each sample.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfde7fd0",
   "metadata": {
    "gather": {
     "logged": 1745565229548
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv_azd import load_azd_env\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from current AZD environment if available\n",
    "load_azd_env(quiet=True)\n",
    "\n",
    "# Load environment variables from local.env file if it exists\n",
    "load_dotenv(dotenv_path=\"local.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8fc57b",
   "metadata": {
    "gather": {
     "logged": 1745565229865
    }
   },
   "outputs": [],
   "source": [
    "# Get Azure AI Foundry credentials from environment variables\n",
    "teacher_model_name = os.getenv('TEACHER_MODEL_NAME')\n",
    "teacher_model_endpoint_url = os.getenv('TEACHER_MODEL_ENDPOINT')\n",
    "teacher_model_api_key = os.getenv('TEACHER_MODEL_KEY')\n",
    "\n",
    "# Print values for debugging (remove in production)\n",
    "print(f\"Teacher Model Name: {teacher_model_name}\")\n",
    "print(f\"Teacher Model Endpoint: {teacher_model_endpoint_url}\")\n",
    "# Don't print the API key for security reasons\n",
    "print(f\"Teacher Model API Key loaded: {'Yes' if teacher_model_api_key else 'No'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5052dde2",
   "metadata": {
    "gather": {
     "logged": 1745565232660
    }
   },
   "outputs": [],
   "source": [
    "pip install azure-ai-inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fb1cbf",
   "metadata": {
    "gather": {
     "logged": 1745565233019
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.ai.inference.models import SystemMessage, UserMessage\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f12ae7",
   "metadata": {
    "gather": {
     "logged": 1745565234026
    }
   },
   "outputs": [],
   "source": [
    "def process_question(question_data):\n",
    "    try:\n",
    "        messages = []\n",
    "        for msg in question_data[\"messages\"]:\n",
    "            if msg[\"role\"] == \"system\":\n",
    "                messages.append(SystemMessage(content=msg[\"content\"]))\n",
    "            elif msg[\"role\"] == \"user\":\n",
    "                messages.append(UserMessage(content=msg[\"content\"]))\n",
    "\n",
    "        response = client.complete(\n",
    "            messages=messages,\n",
    "            model=model_name,\n",
    "            max_tokens=10000  # Reduced since we just need short answers like A, B, C, D, or E\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"question\": question_data[\"messages\"][1][\"content\"],\n",
    "            \"response\": response.choices[0].message.content,\n",
    "            \"full_response\": response\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"question\": question_data[\"messages\"][1][\"content\"] if len(question_data[\"messages\"]) > 1 else \"Error\",\n",
    "            \"response\": f\"Error: {str(e)}\",\n",
    "            \"full_response\": None\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448b00cf",
   "metadata": {
    "gather": {
     "logged": 1745565234583
    }
   },
   "outputs": [],
   "source": [
    "endpoint = f\"{teacher_model_endpoint_url}models\"\n",
    "model_name = teacher_model_name\n",
    "key = teacher_model_api_key\n",
    "client = ChatCompletionsClient(endpoint=endpoint, credential=AzureKeyCredential(key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285ab8b3",
   "metadata": {
    "gather": {
     "logged": 1745565289389
    }
   },
   "outputs": [],
   "source": [
    "# Read the JSONL file and process each question\n",
    "import os\n",
    "results = []\n",
    "\n",
    "# Ensure we're using the correct file path\n",
    "train_data_path = \"./data/train_original_data.jsonl\"\n",
    "\n",
    "# Check if the file exists and has content before proceeding\n",
    "if not os.path.exists(train_data_path):\n",
    "    print(f\"Error: File {train_data_path} does not exist! Please run the previous cells to create it.\")\n",
    "else:\n",
    "    file_size = os.path.getsize(train_data_path)\n",
    "    if file_size == 0:\n",
    "        print(f\"Warning: File {train_data_path} exists but is empty (0 bytes)! Please run the data generation cells first.\")\n",
    "    else:\n",
    "        print(f\"File {train_data_path} exists and has {file_size} bytes.\")\n",
    "        \n",
    "        # Validate first line to ensure it's valid JSON\n",
    "        with open(train_data_path, 'r', encoding='utf-8') as file:\n",
    "            first_line = file.readline().strip()\n",
    "            if not first_line:\n",
    "                print(\"Error: File exists but first line is empty!\")\n",
    "            else:\n",
    "                try:\n",
    "                    # Try parsing the first JSON line\n",
    "                    test_json = json.loads(first_line)\n",
    "                    print(\"JSON validation: First line is valid JSON\")\n",
    "                    \n",
    "                    # Continue with processing all lines\n",
    "                    file.seek(0)  # Go back to start of file\n",
    "                    total_lines = sum(1 for _ in file if _.strip())\n",
    "                    \n",
    "                    print(f\"Processing {total_lines} questions from {train_data_path}\")\n",
    "                    file.seek(0)  # Go back to start of file again\n",
    "                    \n",
    "                    # Initialize tqdm progress bar\n",
    "                    progress_bar = tqdm(total=total_lines, desc=\"Processing questions\", unit=\"question\")\n",
    "                    \n",
    "                    processed_count = 0\n",
    "                    error_count = 0\n",
    "                    \n",
    "                    for i, line in enumerate(file):\n",
    "                        if line.strip():  # Skip empty lines\n",
    "                            try:\n",
    "                                question_data = json.loads(line)\n",
    "                                result = process_question(question_data)\n",
    "                                results.append(result)\n",
    "                                processed_count += 1\n",
    "                                \n",
    "                                # Update progress bar description with latest result\n",
    "                                progress_bar.set_description(f\"Latest answer: {result['response'][:20]}...\")\n",
    "                                progress_bar.update(1)\n",
    "                                \n",
    "                            except json.JSONDecodeError as e:\n",
    "                                error_count += 1\n",
    "                                print(f\"\\nError parsing line {i+1}: {str(e)}\")\n",
    "                                print(f\"Problematic line content: '{line[:100]}...'\")\n",
    "                                progress_bar.update(1)\n",
    "                            except Exception as e:\n",
    "                                error_count += 1\n",
    "                                print(f\"\\nError processing line {i+1}: {str(e)}\")\n",
    "                                progress_bar.update(1)\n",
    "                    \n",
    "                    progress_bar.close()\n",
    "                    \n",
    "                    print(f\"\\nProcessing complete: {processed_count} successful, {error_count} errors\")\n",
    "                    \n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"Error: Invalid JSON in file. First line error: {str(e)}\")\n",
    "                    print(f\"First line content: '{first_line[:100]}...'\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error during file processing: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6de3fb",
   "metadata": {
    "gather": {
     "logged": 1745565289816
    }
   },
   "outputs": [],
   "source": [
    "output_file_path = \"./data/train_data.jsonl\"\n",
    "\n",
    "# Initialize tqdm progress bar for writing results\n",
    "print(f\"Writing {len(results)} processed questions to {output_file_path}\")\n",
    "with open(output_file_path, 'w', encoding='utf-8') as f:\n",
    "    for result in tqdm(results, desc=\"Writing results\", unit=\"record\"):\n",
    "        # Extract just the question content (removing the instruction part)\n",
    "        question_text = result[\"question\"]\n",
    "        # if \"Question: \" in question_text:\n",
    "        #     question_text = question_text.split(\"Question: \")[1].split(\"\\nAnswer Choices:\")[0]\n",
    "\n",
    "        # Create the simplified output format\n",
    "        output_line = {\n",
    "            \"Question\": question_text,\n",
    "            \"Answer\": result[\"response\"]\n",
    "        }\n",
    "\n",
    "        # Write as JSONL (one JSON object per line)\n",
    "        f.write(json.dumps(output_line, ensure_ascii=False) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ccbff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de00c5f2",
   "metadata": {
    "gather": {
     "logged": 1745565290007
    }
   },
   "outputs": [],
   "source": [
    "# Visualize the distillation results\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Count occurrences of each answer\n",
    "answer_counts = {}\n",
    "other_responses = []\n",
    "\n",
    "for result in results:\n",
    "    answer = result['response'].strip()\n",
    "    \n",
    "    # Try to extract just the letter if there's additional text\n",
    "    # This regex matches: A, A., A), (A), (A) text\n",
    "    match = re.search(r'(?:^|\\(|\\s)([A-E])(?:\\)|\\.|\\s|$)', answer, re.IGNORECASE)\n",
    "    if match:\n",
    "        answer = match.group(1).upper()  # Extract just the letter and convert to uppercase\n",
    "        answer_counts[answer] = answer_counts.get(answer, 0) + 1\n",
    "    else:\n",
    "        # For non-standard answers\n",
    "        answer_counts['Other'] = answer_counts.get('Other', 0) + 1\n",
    "        # Store problematic responses for analysis\n",
    "        other_responses.append(answer)\n",
    "\n",
    "# Create a DataFrame for visualization\n",
    "df = pd.DataFrame(list(answer_counts.items()), columns=['Answer', 'Count'])\n",
    "df = df.sort_values('Count', ascending=False)\n",
    "\n",
    "# Create a bar chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(df['Answer'], df['Count'], color='skyblue')\n",
    "plt.title('Distribution of Answers in Distilled Dataset')\n",
    "plt.xlabel('Answer Choice')\n",
    "plt.ylabel('Count')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Add count labels on top of each bar\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
    "            f'{height}', ha='center', va='bottom')\n",
    "\n",
    "# Calculate and display success rate (assuming valid answers are A-E)\n",
    "total_questions = sum(answer_counts.values())\n",
    "valid_answers = sum(answer_counts.get(ans, 0) for ans in ['A', 'B', 'C', 'D', 'E'])\n",
    "success_rate = (valid_answers / total_questions) * 100 if total_questions > 0 else 0\n",
    "\n",
    "plt.figtext(0.5, 0.01, f'Success Rate: {success_rate:.1f}% ({valid_answers}/{total_questions} questions with valid answers)', \n",
    "           ha='center', fontsize=12)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.05, 1, 0.95])\n",
    "plt.show()\n",
    "\n",
    "# Display sample of 'Other' responses if they exist\n",
    "if other_responses:\n",
    "    print(f\"\\nSamples of 'Other' responses (showing up to 10):\")\n",
    "    for i, resp in enumerate(other_responses[:10]):\n",
    "        print(f\"  {i+1}. '{resp}'\")\n",
    "    \n",
    "    # Analyze if there are patterns in the 'Other' responses\n",
    "    lowercase_letters = sum(1 for r in other_responses if r.lower() in ['a', 'b', 'c', 'd', 'e'])\n",
    "    has_period = sum(1 for r in other_responses if re.search(r'[A-Ea-e]\\.', r))\n",
    "    has_explanation = sum(1 for r in other_responses if len(r) > 5)  # Simple heuristic for explanations\n",
    "    \n",
    "    print(f\"\\nAnalysis of {len(other_responses)} 'Other' responses:\")\n",
    "    print(f\"  - Lowercase letters (a-e): {lowercase_letters} ({lowercase_letters/len(other_responses)*100:.1f}%)\")\n",
    "    print(f\"  - Answers with periods (A.): {has_period} ({has_period/len(other_responses)*100:.1f}%)\")\n",
    "    print(f\"  - Likely explanations (>5 chars): {has_explanation} ({has_explanation/len(other_responses)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be286dac",
   "metadata": {
    "gather": {
     "logged": 1745565290281
    }
   },
   "outputs": [],
   "source": [
    "# Analyze potential model bias in answer distribution\n",
    "\n",
    "# Expected distribution (ideally uniform for multiple choice)\n",
    "expected_prob = 0.2  # 20% chance for each of A,B,C,D,E in a uniform distribution\n",
    "expected_counts = {letter: total_questions * expected_prob for letter in ['A', 'B', 'C', 'D', 'E']}\n",
    "\n",
    "# Create a DataFrame for comparing actual vs expected\n",
    "comparison_data = []\n",
    "for letter in ['A', 'B', 'C', 'D', 'E']:\n",
    "    actual = answer_counts.get(letter, 0)\n",
    "    expected = expected_counts[letter]\n",
    "    difference = actual - expected\n",
    "    percent_diff = (difference / expected) * 100 if expected > 0 else 0\n",
    "    comparison_data.append({\n",
    "        'Answer': letter,\n",
    "        'Actual Count': actual,\n",
    "        'Expected Count': expected,\n",
    "        'Difference': difference,\n",
    "        'Percent Difference': percent_diff\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(\"\\nAnalysis of potential answer bias:\")\n",
    "display(comparison_df)\n",
    "\n",
    "# Create a visual comparison\n",
    "plt.figure(figsize=(12, 6))\n",
    "x = range(len(comparison_df))\n",
    "width = 0.35\n",
    "\n",
    "plt.bar([i - width/2 for i in x], comparison_df['Actual Count'], width, label='Actual', color='skyblue')\n",
    "plt.bar([i + width/2 for i in x], comparison_df['Expected Count'], width, label='Expected', color='lightgreen')\n",
    "\n",
    "plt.xlabel('Answer Choice')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Actual vs Expected Answer Distribution')\n",
    "plt.xticks(x, comparison_df['Answer'])\n",
    "plt.legend()\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9954f45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install seaborn scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e318ccd-9825-4b21-a321-89bb5bdfbc22",
   "metadata": {
    "gather": {
     "logged": 1745565290522
    }
   },
   "outputs": [],
   "source": [
    "# Create a confusion matrix to analyze answer patterns\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# We don't have ground truth labels in this dataset, but we can:\n",
    "# 1. Analyze confusion between expected uniform distribution and actual distribution\n",
    "# 2. Alternatively, check if there are patterns in how the model responds to different question types\n",
    "\n",
    "# Approach 1: Create a \"pseudo-confusion matrix\" showing bias toward certain answers\n",
    "# Normalize the counts to get proportions\n",
    "total_valid_answers = sum(answer_counts.get(ans, 0) for ans in ['A', 'B', 'C', 'D', 'E'])\n",
    "pseudo_cm = np.zeros((5, 5))\n",
    "\n",
    "# Fill the diagonal with actual proportions (representing how much the model prefers each answer)\n",
    "for i, letter in enumerate(['A', 'B', 'C', 'D', 'E']):\n",
    "    actual_prop = answer_counts.get(letter, 0) / total_valid_answers if total_valid_answers > 0 else 0\n",
    "    expected_prop = 0.2  # Expected uniform distribution (20% each)\n",
    "    \n",
    "    # The diagonal shows the actual proportion\n",
    "    pseudo_cm[i, i] = actual_prop\n",
    "    \n",
    "    # The off-diagonal elements represent the \"confusion\" - the difference between\n",
    "    # expected and actual distribution\n",
    "    for j in range(5):\n",
    "        if i != j:\n",
    "            pseudo_cm[i, j] = (1 - actual_prop) / 4  # Distribute remaining probability\n",
    "\n",
    "# Plot the pseudo-confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(pseudo_cm, annot=True, fmt='.3f', cmap='Blues',\n",
    "           xticklabels=['A', 'B', 'C', 'D', 'E'],\n",
    "           yticklabels=['A', 'B', 'C', 'D', 'E'])\n",
    "plt.title('Answer Distribution Bias Matrix')\n",
    "plt.xlabel('Predicted Answer')\n",
    "plt.ylabel('Expected Uniform Distribution')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Add text annotation explaining this visualization\n",
    "plt.figtext(0.5, 0.01, \n",
    "            'This matrix shows model bias toward certain answers.\\n'\n",
    "            'Diagonal values represent the proportion of each answer in the results.\\n'\n",
    "            'In an unbiased model, all diagonal values would be close to 0.2 (20%)',\n",
    "            ha='center', fontsize=11, bbox=dict(boxstyle='round,pad=0.5', facecolor='white', alpha=0.8))\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.05, 1, 0.95])\n",
    "plt.show()\n",
    "\n",
    "# Approach 2: If we had access to ground truth or additional features about questions,\n",
    "# we could create an actual confusion matrix here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41839376-61d5-498d-b575-d702356e1521",
   "metadata": {
    "gather": {
     "logged": 1745565315423
    }
   },
   "outputs": [],
   "source": [
    "# Creating a real confusion matrix using ground truth data\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Load the ground truth data\n",
    "train_data_path = \"./data/train_data.jsonl\"\n",
    "\n",
    "# Arrays to store ground truth and predicted labels\n",
    "true_labels = []\n",
    "predicted_labels = []\n",
    "\n",
    "# Load and parse the data\n",
    "print(\"Loading ground truth data from\", train_data_path)\n",
    "with open(train_data_path, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        if line.strip():\n",
    "            try:\n",
    "                data = json.loads(line)\n",
    "                \n",
    "                # Extract the predicted answer from our results variable\n",
    "                question_text = data['Question']\n",
    "                model_answer = data['Answer'].strip()\n",
    "                \n",
    "                # Find the ground truth for this question by parsing the answer choices\n",
    "                # For simplicity, we'll use the letter of the answer\n",
    "                match = re.search(r'\\(([A-E])\\)', model_answer)\n",
    "                if match:\n",
    "                    predicted_letter = match.group(1).upper()\n",
    "                    predicted_labels.append(predicted_letter)\n",
    "                    \n",
    "                    # Try to extract what the actual ground truth is\n",
    "                    # Since we don't have explicit ground truth,\n",
    "                    # we'll use the consistent answer pattern from the commonsense_qa dataset\n",
    "                    # where the correct answer is part of the answer choices\n",
    "                    \n",
    "                    # For this demonstration, we will assume these answers are correct\n",
    "                    # In a real scenario, you would need to match against the ground truth answer key\n",
    "                    true_labels.append(predicted_letter)\n",
    "                \n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error parsing line: {str(e)}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing line: {str(e)}\")\n",
    "\n",
    "# Since we don't have separate ground truth labels in this dataset,\n",
    "# we'll create a more advanced analysis by grouping answers by question type\n",
    "\n",
    "# 1. Create a simple question type classifier based on the first word of the question\n",
    "question_types = []\n",
    "question_type_predicted = []\n",
    "\n",
    "with open(train_data_path, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        if line.strip():\n",
    "            try:\n",
    "                data = json.loads(line)\n",
    "                question_text = data['Question']\n",
    "                \n",
    "                # Extract the actual question from the format\n",
    "                if \"Question: \" in question_text:\n",
    "                    actual_question = question_text.split(\"Question: \")[1].split(\"\\nAnswer Choices:\")[0]\n",
    "                    \n",
    "                    # Simple question type classification based on first word\n",
    "                    first_word = actual_question.strip().split()[0].lower()\n",
    "                    \n",
    "                    # Group question types\n",
    "                    if first_word in ['what', 'which']:\n",
    "                        q_type = 'What/Which'\n",
    "                    elif first_word in ['where']:\n",
    "                        q_type = 'Where'\n",
    "                    elif first_word in ['who']:\n",
    "                        q_type = 'Who'\n",
    "                    elif first_word in ['how']:\n",
    "                        q_type = 'How'\n",
    "                    elif first_word in ['why']:\n",
    "                        q_type = 'Why'\n",
    "                    else:\n",
    "                        q_type = 'Other'\n",
    "                    \n",
    "                    question_types.append(q_type)\n",
    "                    \n",
    "                    # Get the predicted answer letter\n",
    "                    model_answer = data['Answer'].strip()\n",
    "                    match = re.search(r'\\(([A-E])\\)', model_answer)\n",
    "                    if match:\n",
    "                        letter = match.group(1).upper()\n",
    "                        question_type_predicted.append(letter)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error: {str(e)}\")\n",
    "\n",
    "# Count occurrences of each answer by question type\n",
    "q_type_counts = {}\n",
    "for q_type, pred in zip(question_types, question_type_predicted):\n",
    "    if q_type not in q_type_counts:\n",
    "        q_type_counts[q_type] = {'A': 0, 'B': 0, 'C': 0, 'D': 0, 'E': 0}\n",
    "    \n",
    "    q_type_counts[q_type][pred] += 1\n",
    "\n",
    "# Create a better confusion matrix - question type vs answer choice\n",
    "unique_types = sorted(set(question_types))\n",
    "q_type_matrix = np.zeros((len(unique_types), 5))\n",
    "\n",
    "for i, q_type in enumerate(unique_types):\n",
    "    for j, letter in enumerate(['A', 'B', 'C', 'D', 'E']):\n",
    "        q_type_matrix[i, j] = q_type_counts.get(q_type, {}).get(letter, 0)\n",
    "\n",
    "# Normalize by row (question type) to get distribution\n",
    "row_sums = q_type_matrix.sum(axis=1, keepdims=True)\n",
    "row_sums[row_sums == 0] = 1  # Avoid division by zero\n",
    "q_type_matrix_norm = q_type_matrix / row_sums\n",
    "\n",
    "# Plot the question type vs answer choice matrix\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(q_type_matrix_norm, annot=True, fmt='.2f', cmap='YlGnBu',\n",
    "           xticklabels=['A', 'B', 'C', 'D', 'E'],\n",
    "           yticklabels=unique_types)\n",
    "plt.title('Answer Distribution by Question Type (Normalized)')\n",
    "plt.xlabel('Answer Choice')\n",
    "plt.ylabel('Question Type')\n",
    "\n",
    "# Add an explanation of the visualization\n",
    "plt.figtext(0.5, 0.01, \n",
    "            'This matrix shows how answer patterns vary by question type.\\n'\n",
    "            'Each row shows the distribution of answers for a specific question type.\\n'\n",
    "            'An unbiased model would show similar distributions across question types.',\n",
    "            ha='center', fontsize=11, bbox=dict(boxstyle='round,pad=0.5', facecolor='white', alpha=0.8))\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.05, 1, 0.95])\n",
    "plt.show()\n",
    "\n",
    "# Create a second visualization - pattern analysis by question length\n",
    "# Group questions by length and analyze answer patterns\n",
    "question_lengths = []\n",
    "length_predicted = []\n",
    "\n",
    "with open(train_data_path, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        if line.strip():\n",
    "            try:\n",
    "                data = json.loads(line)\n",
    "                question_text = data['Question']\n",
    "                \n",
    "                # Extract the actual question\n",
    "                if \"Question: \" in question_text:\n",
    "                    actual_question = question_text.split(\"Question: \")[1].split(\"\\nAnswer Choices:\")[0]\n",
    "                    \n",
    "                    # Get question length in words\n",
    "                    word_count = len(actual_question.split())\n",
    "                    \n",
    "                    # Group by length\n",
    "                    if word_count < 10:\n",
    "                        length_group = 'Very Short (<10 words)'\n",
    "                    elif word_count < 15:\n",
    "                        length_group = 'Short (10-14 words)'\n",
    "                    elif word_count < 20:\n",
    "                        length_group = 'Medium (15-19 words)'\n",
    "                    else:\n",
    "                        length_group = 'Long (20+ words)'\n",
    "                    \n",
    "                    question_lengths.append(length_group)\n",
    "                    \n",
    "                    # Get the predicted answer letter\n",
    "                    model_answer = data['Answer'].strip()\n",
    "                    match = re.search(r'\\(([A-E])\\)', model_answer)\n",
    "                    if match:\n",
    "                        letter = match.group(1).upper()\n",
    "                        length_predicted.append(letter)\n",
    "                        \n",
    "            except Exception as e:\n",
    "                print(f\"Error: {str(e)}\")\n",
    "\n",
    "# Count occurrences by length group\n",
    "length_counts = {}\n",
    "for length, pred in zip(question_lengths, length_predicted):\n",
    "    if length not in length_counts:\n",
    "        length_counts[length] = {'A': 0, 'B': 0, 'C': 0, 'D': 0, 'E': 0}\n",
    "    \n",
    "    length_counts[length][pred] += 1\n",
    "\n",
    "# Create matrix for length vs answer\n",
    "unique_lengths = ['Very Short (<10 words)', 'Short (10-14 words)', \n",
    "                 'Medium (15-19 words)', 'Long (20+ words)']\n",
    "unique_lengths = [l for l in unique_lengths if l in length_counts]\n",
    "length_matrix = np.zeros((len(unique_lengths), 5))\n",
    "\n",
    "for i, length in enumerate(unique_lengths):\n",
    "    for j, letter in enumerate(['A', 'B', 'C', 'D', 'E']):\n",
    "        length_matrix[i, j] = length_counts.get(length, {}).get(letter, 0)\n",
    "\n",
    "# Normalize by row\n",
    "row_sums = length_matrix.sum(axis=1, keepdims=True)\n",
    "row_sums[row_sums == 0] = 1  # Avoid division by zero\n",
    "length_matrix_norm = length_matrix / row_sums\n",
    "\n",
    "# Plot the question length vs answer choice matrix\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(length_matrix_norm, annot=True, fmt='.2f', cmap='YlOrRd',\n",
    "           xticklabels=['A', 'B', 'C', 'D', 'E'],\n",
    "           yticklabels=unique_lengths)\n",
    "plt.title('Answer Distribution by Question Length (Normalized)')\n",
    "plt.xlabel('Answer Choice')\n",
    "plt.ylabel('Question Length Group')\n",
    "\n",
    "# Add an explanation of the visualization\n",
    "plt.figtext(0.5, 0.01, \n",
    "            'This matrix shows how answer patterns vary by question length.\\n'\n",
    "            'Differences between rows may indicate biases in how the model handles questions of different complexity.',\n",
    "            ha='center', fontsize=11, bbox=dict(boxstyle='round,pad=0.5', facecolor='white', alpha=0.8))\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.05, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0a46c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python38-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3.8 - AzureML",
   "language": "python",
   "name": "python38-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   },
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
