{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "489906e9",
   "metadata": {},
   "source": [
    "# Local Inference with Azure Foundry Local\n",
    "\n",
    "**Placeholder** This notebook demonstrates how to use Azure Foundry Local to run inference with your optimized model on your local machine. Azure Foundry Local provides a simple, containerized way to serve and interact with large language models, including those you have fine-tuned and exported from Azure ML.\n",
    "\n",
    "## What You'll Learn\n",
    "- How to install and configure Azure Foundry Local\n",
    "- How to launch a local model server using Foundry\n",
    "- How to send prompts and receive completions from your model\n",
    "- How to use the Foundry Python SDK for local inference\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35152bf5",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Prepare Your Model and Config for Foundry Local\n",
    "\n",
    "- Ensure your model and any adapters (such as LoRA) are exported in a format supported by Foundry Local (e.g., ONNX, GGUF, or HuggingFace Transformers format).\n",
    "- Place your model files in a directory, e.g., `./LocalFoundryEnv/`.\n",
    "- Create or update an `inference_model.json` config file in that directory, following the [Foundry Local model config guide](https://github.com/microsoft/Foundry-Local/blob/main/docs/model-config.md).\n",
    "\n",
    "Example `inference_model.json`:\n",
    "```\n",
    "{\n",
    "  \"model_format\": \"onnx\",\n",
    "  \"model_path\": \"./phi-4-mini-onnx-int4-cpu/1/model\",\n",
    "  \"adapter_path\": \"./phi-4-mini-onnx-int4-cpu/1/model/adapter_weights.onnx_adapter\",\n",
    "  \"chat_template\": \"You are a helpful assistant. Your output should only be one of the five choices: 'A', 'B', 'C', 'D', or 'E'.\"\n",
    "}\n",
    "```\n",
    "> Tip: If you used 05.Local_Download.ipynb, your model files should already be in a suitable directory. Just add or edit the config file as above.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f180b900",
   "metadata": {},
   "source": [
    "## 3. Install the Foundry Local if not already installed\n",
    "\n",
    "Download AI Foundry Local for your platform from the releases page.\n",
    "\n",
    "Install the package by following the on-screen prompts. After installation, access the tool via command line with foundry."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e9f264",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Running Your First Model\n",
    "- Open a command prompt or terminal window.\n",
    "- Run a model using the following command:\n",
    "- foundry model run deepseek-r1-1.5b-cpu\n",
    "\n",
    "This command will:\n",
    "\n",
    "- Download the model to your local disk\n",
    "- Load the model into your device\n",
    "- Start a chat interface\n",
    "\n",
    "ðŸ’¡ TIP: Replace deepseek-r1-1.5b-cpu with any model from the catalog. Use foundry model list to see available models. In our case we will replace deepseek-r-1-5b-cpu with our local downloaded model from 05.local_download.ipynb. If you used the default settings in 05.Local_Download.ipynb, the model name:\"fine-tuning-phi-4-mini-onnx-int4-cpu\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a41f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the Foundry SDK if not already installed\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "def install_package(package):\n",
    "    try:\n",
    "        __import__(package)\n",
    "        print(f\"{package} is already installed.\")\n",
    "    except ImportError:\n",
    "        print(f\"Installing {package}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "install_package(\"foundry\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec8f770",
   "metadata": {},
   "source": [
    "## 5. Connect to the Local Foundry Server and Run Inference\n",
    "\n",
    "Now, use the Foundry SDK to connect to your local server and send prompts for inference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d030f033",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from foundry import FoundryClient\n",
    "\n",
    "# Connect to the local Foundry server\n",
    "client = FoundryClient(base_url=\"http://localhost:5000\")\n",
    "\n",
    "# Example multiple-choice question\n",
    "question = \"Which planet is closest to the Sun?\"\n",
    "choices = {\n",
    "    \"A\": \"Venus\",\n",
    "    \"B\": \"Earth\",\n",
    "    \"C\": \"Mercury\",\n",
    "    \"D\": \"Mars\",\n",
    "    \"E\": \"Jupiter\"\n",
    "}\n",
    "choice_text = \"\\n\".join([f\"({k}) {v}\" for k, v in choices.items()])\n",
    "prompt = (\n",
    "    \"Answer the following multiple-choice question by selecting the correct option.\\n\\n\"\n",
    "    f\"Question: {question}\\nAnswer Choices:\\n{choice_text}\"\n",
    ")\n",
    "\n",
    "system_prompt = \"You are a helpful assistant. Your output should only be one of the five choices: 'A', 'B', 'C', 'D', or 'E'.\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"local\",  # 'local' is the default model name for Foundry Local\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ],\n",
    "    max_tokens=10,\n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "print(\"Model response:\", response.choices[0].message.content)\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"markdown\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37f3e5f",
   "metadata": {},
   "source": [
    "## 6. Try Your Own Questions\n",
    "\n",
    "You can now use the `client` object to send any prompt to your local model. Try with your own multiple-choice questions or other tasks supported by your model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c6790f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ask_foundry_mcq(client, question, choices):\n",
    "    choice_text = \"\\n\".join([f\"({k}) {v}\" for k, v in choices.items()])\n",
    "    prompt = (\n",
    "        \"Answer the following multiple-choice question by selecting the correct option.\\n\\n\"\n",
    "        f\"Question: {question}\\nAnswer Choices:\\n{choice_text}\"\n",
    "    )\n",
    "    system_prompt = \"You are a helpful assistant. Your output should only be one of the five choices: 'A', 'B', 'C', 'D', or 'E'.\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"local\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        max_tokens=10,\n",
    "        temperature=0.0\n",
    "    )\n",
    "    print(\"Q:\", question)\n",
    "    print(\"A:\", response.choices[0].message.content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31011f76",
   "metadata": {},
   "source": [
    "# Example usage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbe0cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ask_foundry_mcq(\n",
    "    client,\n",
    "    \"What is the capital of France?\",\n",
    "    {\n",
    "        \"A\": \"Berlin\",\n",
    "        \"B\": \"London\",\n",
    "        \"C\": \"Paris\",\n",
    "        \"D\": \"Madrid\",\n",
    "        \"E\": \"Rome\"\n",
    "    }\n",
    ")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af86a2d",
   "metadata": {},
   "source": [
    "## 7. Next Steps\n",
    "\n",
    "- Explore more advanced prompt engineering and system instructions\n",
    "- Benchmark your model's performance locally\n",
    "- Integrate the local Foundry server into your applications\n",
    "- For more details, see the [Foundry Local documentation](https://github.com/microsoft/Foundry-Local/tree/main/docs)\n",
    "\n",
    "\n",
    "**Congratulations!** You have successfully run local inference with your optimized model using Azure Foundry Local.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
