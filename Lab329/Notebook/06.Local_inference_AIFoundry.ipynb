{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "489906e9",
   "metadata": {},
   "source": [
    "# Local Inference with Azure Foundry Local\n",
    "\n",
    "**Placeholder** This notebook demonstrates how to use Azure Foundry Local to run inference with your optimized model on your local machine. Azure Foundry Local provides a simple, containerized way to serve and interact with large language models, including those you have fine-tuned and exported from Azure ML.\n",
    "\n",
    "## What You'll Learn\n",
    "- How to install and configure Azure Foundry Local\n",
    "- How to launch a local model server using Foundry\n",
    "- How to send prompts and receive completions from your model\n",
    "- How to use the Foundry Python SDK for local inference\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf94568",
   "metadata": {},
   "source": [
    "## 1. Prerequisites\n",
    "- Completed the previous notebooks and have a model exported in ONNX or supported format (see 05.Local_Download.ipynb)\n",
    "- Windows, macOS, or Linux\n",
    "- Python 3.10+ installed locally\n",
    "- Sufficient disk space and memory for your model\n",
    "\n",
    "## References\n",
    "- [Azure Foundry Local Documentation](https://github.com/microsoft/Foundry-Local/tree/main/docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35152bf5",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Prepare Your Model and Config for Foundry Local\n",
    "\n",
    "- Ensure your model and any adapters (such as LoRA) are exported in a format supported by Foundry Local (e.g., ONNX, GGUF, or HuggingFace Transformers format).\n",
    "- Place your model files in a directory, e.g., `./LocalFoundryEnv/`.\n",
    "- Create or update an `inference_model.json` config file in that directory, following the [Foundry Local model config guide](https://github.com/microsoft/Foundry-Local/blob/main/docs/model-config.md).\n",
    "\n",
    "Example `inference_model.json`:\n",
    "```\n",
    "{\n",
    "  \"model_format\": \"onnx\",\n",
    "  \"model_path\": \"./phi-4-mini-onnx-int4-cpu/1/model\",\n",
    "  \"adapter_path\": \"./phi-4-mini-onnx-int4-cpu/1/model/adapter_weights.onnx_adapter\",\n",
    "  \"chat_template\": \"You are a helpful assistant. Your output should only be one of the five choices: 'A', 'B', 'C', 'D', or 'E'.\"\n",
    "}\n",
    "```\n",
    "> Tip: If you used 05.Local_Download.ipynb, your model files should already be in a suitable directory. Just add or edit the config file as above.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f180b900",
   "metadata": {},
   "source": [
    "## 3. Install the Foundry Local if not already installed\n",
    "\n",
    "Download AI Foundry Local for your platform from the releases page.\n",
    "\n",
    "Install the package by following the on-screen prompts. After installation, access the tool via command line with foundry."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e9f264",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Running Your First Model\n",
    "- Open a command prompt or terminal window.\n",
    "- Run a model using the following command:\n",
    "- foundry model run deepseek-r1-1.5b-cpu\n",
    "\n",
    "This command will:\n",
    "\n",
    "- Download the model to your local disk\n",
    "- Load the model into your device\n",
    "- Start a chat interface\n",
    "\n",
    "ðŸ’¡ TIP: Replace deepseek-r1-1.5b-cpu with any model from the catalog. Use foundry model list to see available models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367d0cb1",
   "metadata": {},
   "source": [
    "## 5. Explore Foundry Local CLI commands\n",
    "The foundry CLI is structured into several categories:\n",
    "\n",
    "- Model: Commands related to managing and running models\n",
    "- Service: Commands for managing the AI Foundry Local service\n",
    "- Cache: Commands for managing the local cache where models are stored\n",
    "- To see all available commands, use the help option: `foundry --help`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37f3e5f",
   "metadata": {},
   "source": [
    "## 6. Try Your Own Questions\n",
    "\n",
    "You can now use the `client` object to send any prompt to your local model. Try with your own multiple-choice questions or other tasks supported by your model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af86a2d",
   "metadata": {},
   "source": [
    "## 7. Next Steps\n",
    "\n",
    "- Explore more advanced prompt engineering and system instructions\n",
    "- Benchmark your model's performance locally\n",
    "- Integrate the local Foundry server into your applications\n",
    "- For more details, see the [Foundry Local documentation](https://github.com/microsoft/Foundry-Local/tree/main/docs)\n",
    "\n",
    "\n",
    "**Congratulations!** You have successfully run local inference with your optimized model using Azure Foundry Local.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
