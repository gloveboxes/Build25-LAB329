{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "489906e9",
   "metadata": {},
   "source": [
    "# Local Inference with Azure Foundry Local\n",
    "\n",
    "This notebook demonstrates how to use Azure Foundry Local to run inference with your optimized model on your local machine. Azure Foundry Local provides a simple, containerized way to serve and interact with large language models, including those you have fine-tuned and exported from Azure ML.\n",
    "\n",
    "## What You'll Learn\n",
    "- How to install and configure Azure Foundry Local\n",
    "- How to launch a local model server using Foundry\n",
    "- How to send prompts and receive completions from your model\n",
    "- How to use the Foundry Python SDK for local inference\n",
    "\n",
    "## Prerequisites\n",
    "- Completed the previous notebooks and have a model exported in ONNX or supported format (see 05.Local_Download.ipynb)\n",
    "- Windows, macOS, or Linux with Docker installed\n",
    "- Python 3.10+ installed locally\n",
    "- Sufficient disk space and memory for your model\n",
    "\n",
    "## References\n",
    "- [Azure Foundry Local Documentation](https://github.com/microsoft/Foundry-Local/tree/main/docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35152bf5",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Prepare Your Model and Config for Foundry Local\n",
    "\n",
    "- Ensure your model and any adapters (such as LoRA) are exported in a format supported by Foundry Local (e.g., ONNX, GGUF, or HuggingFace Transformers format).\n",
    "- Place your model files in a directory, e.g., `./LocalFoundryEnv/`.\n",
    "- Create or update an `inference_model.json` config file in that directory, following the [Foundry Local model config guide](https://github.com/microsoft/Foundry-Local/blob/main/docs/model-config.md).\n",
    "\n",
    "Example `inference_model.json`:\n",
    "```\n",
    "{\n",
    "  \"model_format\": \"onnx\",\n",
    "  \"model_path\": \"./phi-4-mini-onnx-int4-cpu/1/model\",\n",
    "  \"adapter_path\": \"./phi-4-mini-onnx-int4-cpu/1/model/adapter_weights.onnx_adapter\",\n",
    "  \"chat_template\": \"You are a helpful assistant. Your output should only be one of the five choices: 'A', 'B', 'C', 'D', or 'E'.\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8546e662",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "> Tip: If you used 05.Local_Download.ipynb, your model files should already be in a suitable directory. Just add or edit the config file as above.\n",
    "\n",
    "## 3. Launch Foundry Local Model Server\n",
    "\n",
    "Start the Foundry Local server using Docker, mounting your model directory. Replace `<absolute_path_to_LocalFoundryEnv>` with the full path to your model config and files.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c025cc",
   "metadata": {
    "vscode": {
     "languageId": "pwsh"
    }
   },
   "outputs": [],
   "source": [
    "docker run --rm -it -p 5000:5000 `\n",
    "  -v <absolute_path_to_LocalFoundryEnv>:/workspace `\n",
    "  mcr.microsoft.com/azureml/llm-foundry-local:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856b6074",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "- The server will start and listen on `http://localhost:5000/v1/completions`\n",
    "- You can now interact with your model using the OpenAI-compatible API or the Python SDK\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "## 4. Install the Foundry Python SDK\n",
    "\n",
    "To interact with your local Foundry server from Python, install the SDK:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e9f264",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install foundry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a41f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the Foundry SDK if not already installed\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "def install_package(package):\n",
    "    try:\n",
    "        __import__(package)\n",
    "        print(f\"{package} is already installed.\")\n",
    "    except ImportError:\n",
    "        print(f\"Installing {package}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "install_package(\"foundry\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec8f770",
   "metadata": {},
   "source": [
    "## 5. Connect to the Local Foundry Server and Run Inference\n",
    "\n",
    "Now, use the Foundry SDK to connect to your local server and send prompts for inference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d030f033",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from foundry import FoundryClient\n",
    "\n",
    "# Connect to the local Foundry server\n",
    "client = FoundryClient(base_url=\"http://localhost:5000\")\n",
    "\n",
    "# Example multiple-choice question\n",
    "question = \"Which planet is closest to the Sun?\"\n",
    "choices = {\n",
    "    \"A\": \"Venus\",\n",
    "    \"B\": \"Earth\",\n",
    "    \"C\": \"Mercury\",\n",
    "    \"D\": \"Mars\",\n",
    "    \"E\": \"Jupiter\"\n",
    "}\n",
    "choice_text = \"\\n\".join([f\"({k}) {v}\" for k, v in choices.items()])\n",
    "prompt = (\n",
    "    \"Answer the following multiple-choice question by selecting the correct option.\\n\\n\"\n",
    "    f\"Question: {question}\\nAnswer Choices:\\n{choice_text}\"\n",
    ")\n",
    "\n",
    "system_prompt = \"You are a helpful assistant. Your output should only be one of the five choices: 'A', 'B', 'C', 'D', or 'E'.\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"local\",  # 'local' is the default model name for Foundry Local\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ],\n",
    "    max_tokens=10,\n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "print(\"Model response:\", response.choices[0].message.content)\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"markdown\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37f3e5f",
   "metadata": {},
   "source": [
    "## 6. Try Your Own Questions\n",
    "\n",
    "You can now use the `client` object to send any prompt to your local model. Try with your own multiple-choice questions or other tasks supported by your model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c6790f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ask_foundry_mcq(client, question, choices):\n",
    "    choice_text = \"\\n\".join([f\"({k}) {v}\" for k, v in choices.items()])\n",
    "    prompt = (\n",
    "        \"Answer the following multiple-choice question by selecting the correct option.\\n\\n\"\n",
    "        f\"Question: {question}\\nAnswer Choices:\\n{choice_text}\"\n",
    "    )\n",
    "    system_prompt = \"You are a helpful assistant. Your output should only be one of the five choices: 'A', 'B', 'C', 'D', or 'E'.\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"local\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        max_tokens=10,\n",
    "        temperature=0.0\n",
    "    )\n",
    "    print(\"Q:\", question)\n",
    "    print(\"A:\", response.choices[0].message.content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31011f76",
   "metadata": {},
   "source": [
    "# Example usage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbe0cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ask_foundry_mcq(\n",
    "    client,\n",
    "    \"What is the capital of France?\",\n",
    "    {\n",
    "        \"A\": \"Berlin\",\n",
    "        \"B\": \"London\",\n",
    "        \"C\": \"Paris\",\n",
    "        \"D\": \"Madrid\",\n",
    "        \"E\": \"Rome\"\n",
    "    }\n",
    ")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af86a2d",
   "metadata": {},
   "source": [
    "## 7. Next Steps\n",
    "\n",
    "- Explore more advanced prompt engineering and system instructions\n",
    "- Benchmark your model's performance locally\n",
    "- Integrate the local Foundry server into your applications\n",
    "- For more details, see the [Foundry Local documentation](https://github.com/microsoft/Foundry-Local/tree/main/docs)\n",
    "\n",
    "\n",
    "**Congratulations!** You have successfully run local inference with your optimized model using Azure Foundry Local.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
