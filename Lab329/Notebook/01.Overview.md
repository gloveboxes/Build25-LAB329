# Knowledge Distillation Preparation & Data Generation

This notebook ([01.AzureML_DistillationByMAI.ipynb](01.AzureML_DistillationByMAI.ipynb)) implements the first phase of our model distillation pipeline: preparing data from a Hugging Face dataset and using a teacher model to generate high-quality responses for training a student model.

## Purpose

This notebook demonstrates the initial phase of knowledge distillation by:
1. Loading a dataset from Hugging Face
2. Processing it into a suitable format for AI model consumption
3. Using a teacher model (Azure OpenAI or similar LLM) to generate answers
4. Saving these responses for subsequent training of a smaller model

## Workflow Overview

1. **Environment Setup**: Installation of essential libraries and configuration of authentication
2. **Dataset Handling**: Loading and processing the commonsense QA dataset
3. **Data Preparation**: Formatting questions for model consumption
4. **Teacher Model Inference**: Generating high-quality answers using Azure AI
5. **Results Processing**: Organizing the output for student model training

## Technical Components

### Environment Setup
- Installation of required Python packages:
  - `python-dotenv` for environment variable management
  - `datasets` for Hugging Face dataset access
- Loading environment variables for secure authentication with Azure AI services

### Dataset Handling
- Uses Hugging Face's `datasets` library to load the "tau/commonsense_qa" dataset
- Creates a custom `CQnAHuggingFaceInputDataset` class to handle dataset loading and processing
- Implements sample selection capability for training and validation data
- Loads a configurable sample size (set to 100 examples each for training and validation)

### Data Preparation
- Formats the common sense questions into a consistent structure
- Creates a JSONL file with questions structured as chat messages
- Implements a specific system prompt that instructs the model to respond with one of five multiple-choice answers (A, B, C, D, or E)
- Formats input with question text and formatted answer choices

### Teacher Model Inference
- Connects to an Azure AI service endpoint using environment variables
- Uses the Azure AI Inference SDK's `ChatCompletionsClient` to make API calls to the teacher model
- Processes each question through the teacher model to generate answers
- Implements error handling for API calls and parsing responses
- Tracks progress of batch processing with informative console output

### Results Processing
- Collects responses from the teacher model
- Formats the results into a new JSONL file with question-answer pairs
- Extracts essential information from complex response objects
- Saves the processed data (./data/train_data.jsonl) for use in the next stage of the distillation process

## Code Highlights

```python
# System prompt definition
system_prompt = "You are a helpful assistant. Your output should only be one of the five choices: 'A', 'B', 'C', 'D', or 'E'."

# Azure AI model invocation
client = ChatCompletionsClient(endpoint=endpoint, credential=AzureKeyCredential(key))

# Processing function for each question
def process_question(question_data):
    try:
        messages = []
        for msg in question_data["messages"]:
            if msg["role"] == "system":
                messages.append(SystemMessage(content=msg["content"]))
            elif msg["role"] == "user":
                messages.append(UserMessage(content=msg["content"]))

        response = client.complete(
            messages=messages,
            model=model_name,
            max_tokens=100
        )

        return {
            "question": question_data["messages"][1]["content"],
            "response": response.choices[0].message.content,
            "full_response": response
        }
    except Exception as e:
        return {
            "question": question_data["messages"][1]["content"] if len(question_data["messages"]) > 1 else "Error",
            "response": f"Error: {str(e)}",
            "full_response": None
        }
```

## Benefits of This Approach

1. **Quality Data Generation**: Leverages a powerful teacher model to create high-quality training data
2. **Focused Task Learning**: By using a specific prompt structure, ensures consistent response format
3. **Efficiency**: Processes a manageable dataset size to demonstrate the concept without excessive compute
4. **Reproducibility**: Structured approach makes the process repeatable with different datasets
5. **Pipeline Integration**: Output is formatted for seamless use in the next notebook (fine-tuning)

This notebook serves as the foundation for the distillation process by creating a dataset of expert (teacher model) responses that will be used to train a smaller, more efficient model while maintaining comparable performance on specific tasks. The approach enables knowledge transfer from large, powerful models to more efficient models suitable for deployment in resource-constrained environments.