# 07. Local Inference with Azure Foundry Local â€” Overview

This notebook guides you through running local inference with your optimized model using Azure Foundry Local. After downloading and preparing your model in previous steps, you will use Foundry Local to serve and interact with your model on your own machine.

## What You'll Learn
- How to install and configure Azure Foundry Local
- How to prepare your model and configuration for local serving
- How to launch a local model server using Foundry
- How to interact with your model using the Foundry CLI and Python SDK
- How to send prompts and receive completions from your model

## Prerequisites
- Completion of previous notebooks, with a model exported in ONNX or supported format (see 05.Local_Download.ipynb)
- Windows, macOS, or Linux
- Python 3.10+ installed locally
- Sufficient disk space and memory for your model

## Key Steps
1. **Prepare Your Model and Config**
   - Ensure your model and any adapters are in a supported format (e.g., ONNX).
   - Place your model files in a directory (e.g., `./LocalFoundryEnv/`).
   - Create or update an `inference_model.json` config file in that directory.
2. **Install Azure Foundry Local**
   - Download and install Foundry Local for your platform.
   - Access the tool via command line with `foundry`.
3. **Run Your Model Locally**
   - Use the `foundry model run` command to start your model.
   - The tool will download, load, and serve your model, providing a chat interface.
4. **Explore Foundry CLI Commands**
   - Use commands like `foundry model list` and `foundry --help` to explore capabilities.
5. **Try Your Own Questions**
   - Interact with your model using the CLI or Python SDK.

## Next Steps
- Experiment with prompt engineering and system instructions
- Benchmark your model's performance locally
- Integrate the local Foundry server into your own applications
- For more details, see the [Azure Foundry Local documentation](https://github.com/microsoft/Foundry-Local/tree/main/docs)

---
**Congratulations!** You are now ready to run and test your optimized model locally using Azure Foundry Local.
