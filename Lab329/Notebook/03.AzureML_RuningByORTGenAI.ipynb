{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "002cd1e6",
   "metadata": {},
   "source": [
    "# Testing Your Optimized Model with ONNX Runtime GenAI\n",
    "\n",
    "This notebook demonstrates how to use your fine-tuned and optimized model for inference using ONNX Runtime GenAI. We'll load both the model and adapter created in the previous notebook and test it on sample questions.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "- How to load an ONNX-optimized language model\n",
    "- How to apply a LoRA adapter for fine-tuned capabilities\n",
    "- How to run efficient inference using ONNX Runtime GenAI\n",
    "- How to format inputs and process outputs\n",
    "- How to test your model on sample questions\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Completed the previous notebooks:\n",
    "  - `01.AzureML_Distillation.ipynb` (generated training data)\n",
    "  - `02.AzureML_FineTuningAndConvertByMSOlive.ipynb` (fine-tuned and optimized the model) \n",
    "- Successfully created model files in `models/phi-4-mini/onnx/`\n",
    "- Python environment with necessary libraries (which we'll install)\n",
    "\n",
    "## Setup Instructions\n",
    "\n",
    "1. **Azure Authentication**: Ensure you're logged in to Azure using `az login --use-device-code` in a terminal\n",
    "2. **Kernel Selection**: Change the Jupyter kernel to **\"Python 3.10 Azure ML\"** using the selector in the top right\n",
    "3. **Check Files**: Verify your model files exist in the path shown in this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9f2007",
   "metadata": {},
   "source": [
    "## Initial Setup\n",
    "\n",
    "Before proceeding with this notebook, ensure you've completed these important setup steps:\n",
    "\n",
    "1. **Azure Authentication**: Run `az login --use-device-code` in a terminal to authenticate with Azure\n",
    "\n",
    "2. **Kernel Selection**: Select the **\"Python 3.10 Azure ML\"** kernel from the dropdown menu in the top-right corner of this notebook. This kernel has the necessary libraries pre-installed.\n",
    "\n",
    "3. **File Verification**: Confirm that the model files created in the previous notebook exist in the `/models/phi-4-mini/onnx/` directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800c1c37",
   "metadata": {},
   "source": [
    "## 1. Install ONNX Runtime\n",
    "\n",
    "First, we'll install ONNX Runtime, which is the inference engine we'll use to run our optimized model. ONNX (Open Neural Network Exchange) is an open standard for representing machine learning models, and ONNX Runtime is a high-performance inference engine for those models.\n",
    "\n",
    "We're installing a specific version (1.21.0) to ensure compatibility with our other components. The `-U` flag ensures we get an upgrade if an older version is already installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2f4a9b-be22-4f45-a421-35d501ab4a18",
   "metadata": {
    "gather": {
     "logged": 1744965103069
    }
   },
   "outputs": [],
   "source": [
    "! pip install  onnxruntime==1.21.1 -U"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8e7d74",
   "metadata": {},
   "source": [
    "## 2. Import Required Libraries\n",
    "\n",
    "Now we'll import the necessary libraries for running our optimized model:\n",
    "\n",
    "- **onnxruntime_genai (og)**: A specialized version of ONNX Runtime designed specifically for generative AI models, providing efficient inference for transformer-based language models\n",
    "\n",
    "- **numpy (np)**: A fundamental package for scientific computing in Python, which we'll use for numerical operations\n",
    "\n",
    "- **os**: The standard Python module for interacting with the operating system, which we'll use for file path operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81c41fc",
   "metadata": {
    "gather": {
     "logged": 1744965109605
    }
   },
   "outputs": [],
   "source": [
    "import onnxruntime_genai as og\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3ad3b9",
   "metadata": {},
   "source": [
    "## 3. Check Current Working Directory\n",
    "\n",
    "Before loading our model, we'll check where we're currently located in the filesystem. This helps ensure we use the correct relative paths when loading model files.\n",
    "\n",
    "The code uses the `os.getcwd()` function to get the current working directory and prints it. This information is useful for debugging path-related issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03481afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "current_path = os.getcwd()  # Gets the current working directory\n",
    "print(f\"Current Path: {current_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce43a8bb",
   "metadata": {},
   "source": [
    "## 4. Set Model Folder Path\n",
    "\n",
    "Here we define the path to our ONNX-optimized model files. This should point to the directory where our model was saved in the previous notebook after the optimization process.\n",
    "\n",
    "The path `./models/phi-4-mini/onnx/model` is a relative path starting from our current working directory. This folder should contain all the necessary ONNX model files, including the main model weights and configuration files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c437d8",
   "metadata": {
    "gather": {
     "logged": 1744965112312
    }
   },
   "outputs": [],
   "source": [
    "model_folder = \"models/phi-4-mini/onnx/model\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2612d2",
   "metadata": {},
   "source": [
    "## 5. Load the ONNX Model\n",
    "\n",
    "This is where we load our optimized model into memory using ONNX Runtime GenAI. The `og.Model()` function creates a model object by loading the files from our specified model folder.\n",
    "\n",
    "During this step, the following happens:\n",
    "1. ONNX Runtime loads the model architecture and weights\n",
    "2. The model is prepared for inference\n",
    "3. Any optimizations made during the ONNX conversion are applied\n",
    "\n",
    "This model loading step may take a few moments depending on the size of the model and your hardware capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc07d165",
   "metadata": {
    "gather": {
     "logged": 1744965152627
    }
   },
   "outputs": [],
   "source": [
    "model = og.Model(model_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbdda339",
   "metadata": {},
   "source": [
    "## 6. Load the LoRA Adapter\n",
    "\n",
    "Now we load the LoRA (Low-Rank Adaptation) adapter that contains the fine-tuned weights from our knowledge distillation process. This adapter is what gives our model its specialized knowledge for answering multiple-choice questions.\n",
    "\n",
    "The process works as follows:\n",
    "1. First, we create an `Adapters` object associated with our base model\n",
    "2. Then we load the specific adapter file from the path `./models/phi-4-mini/onnx/model/adapter_weights.onnx_adapter`\n",
    "3. We give it the name \"qa_choice\" which we'll refer to later when we activate it\n",
    "\n",
    "This approach allows us to keep the base model unchanged while applying our specialized fine-tuning through the adapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597bf101",
   "metadata": {
    "gather": {
     "logged": 1744965153242
    }
   },
   "outputs": [],
   "source": [
    "adapters = og.Adapters(model)\n",
    "adapters.load('./models/phi-4-mini/onnx/model/adapter_weights.onnx_adapter', \"qa_choice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad694b1",
   "metadata": {},
   "source": [
    "## 7. Set Up the Tokenizer\n",
    "\n",
    "Here we create a tokenizer for our model, which is responsible for converting text into tokens (numerical representations) that the model can understand.\n",
    "\n",
    "1. First, we create a tokenizer associated with our model using `og.Tokenizer(model)`\n",
    "2. Then we create a tokenizer stream, which will help us decode generated tokens back to text\n",
    "\n",
    "The tokenizer handles all the text preprocessing needed for our model, ensuring that inputs are properly formatted and outputs are correctly decoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65e7217",
   "metadata": {
    "gather": {
     "logged": 1744965153908
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = og.Tokenizer(model)\n",
    "tokenizer_stream = tokenizer.create_stream()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c18e44c",
   "metadata": {},
   "source": [
    "## 8. Configure Generation Settings\n",
    "\n",
    "Here we configure the settings that will control how our model generates text. These parameters affect the behavior of the text generation process:\n",
    "\n",
    "- **max_length**: Sets the maximum number of tokens that the model will generate (102 in this case)\n",
    "\n",
    "- **past_present_share_buffer**: When set to False, the model uses separate memory buffers for past and present states, which can be more memory-intensive but sometimes more stable\n",
    "\n",
    "These settings help balance the quality of generation with computational efficiency. For our multiple-choice question answering task, we keep these settings relatively simple since we only need short answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a92eb5d",
   "metadata": {
    "gather": {
     "logged": 1744965154018
    }
   },
   "outputs": [],
   "source": [
    "search_options = {}\n",
    "search_options['max_length'] = 120\n",
    "search_options['past_present_share_buffer'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9322170c",
   "metadata": {},
   "source": [
    "## 9. Define a Sample Test Question\n",
    "\n",
    "Now we'll define a sample multiple-choice question to test our model. This question follows the same format as the questions we used to train our model in the previous notebooks.\n",
    "\n",
    "The question includes:\n",
    "1. A clear instruction about answering a multiple-choice question\n",
    "2. The question itself about sanctions against a school\n",
    "3. Five possible answer choices labeled A through E\n",
    "\n",
    "We'll use this input to test whether our fine-tuned model can correctly understand and respond to multiple-choice questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a9f4fc",
   "metadata": {
    "gather": {
     "logged": 1744965154126
    }
   },
   "outputs": [],
   "source": [
    "input = \"Answer the following multiple-choice question by selecting the correct option.\\n\\nQuestion: Sammy wanted to go to where the people were.  Where might he go?\\nAnswer Choices:\\n(A) race track\\n(B) populated areas\\n(C) the desert\\n(D) apartment\\n(E) roadblock\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d635d48",
   "metadata": {},
   "source": [
    "## 10. Define the Chat Template\n",
    "\n",
    "Here we define a chat template that formats our input for the model. This template follows the specific format that our model was fine-tuned with and includes:\n",
    "\n",
    "1. **`</s>`**: An end-of-sequence token to mark the start of the conversation\n",
    "\n",
    "2. **System message**: Instructions to the model that it should only respond with one of the five choices (A-E)\n",
    "\n",
    "3. **`<|end|>`, `<|user|>`, `<|assistant|>`**: Special tokens that define different parts of the conversation (end of a message, user input, and assistant response)\n",
    "\n",
    "4. **`{input}`**: A placeholder that will be replaced with our question\n",
    "\n",
    "This formatting is crucial for the model to properly understand its role and the task at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deec901b",
   "metadata": {
    "gather": {
     "logged": 1744965154242
    }
   },
   "outputs": [],
   "source": [
    "chat_template = \"<|system|>You are a helpful assistant. Your output should only be one of the five choices: 'A', 'B', 'C', 'D', or 'E'.<|end|><|user|>{input}<|end|><|assistant|>\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f6baf8",
   "metadata": {},
   "source": [
    "## 11. Format the Full Prompt\n",
    "\n",
    "This step combines our chat template with the actual question. The `format()` method replaces the `{input}` placeholder in our template with the multiple-choice question we defined earlier.\n",
    "\n",
    "The result is a complete, properly formatted prompt that follows the structure our model expects, with system instructions, user question, and a marker indicating where the model should start its response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c5e734",
   "metadata": {
    "gather": {
     "logged": 1744965154345
    }
   },
   "outputs": [],
   "source": [
    "prompt = f'{chat_template.format(input=input)}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bbac3f",
   "metadata": {},
   "source": [
    "## 12. Tokenize the Input\n",
    "\n",
    "Before we can feed our prompt to the model, we need to convert it from text into tokens (numerical representations that the model can process). This step uses the tokenizer we set up earlier to encode our formatted prompt.\n",
    "\n",
    "The `tokenizer.encode()` function splits the text into tokens and converts them to their corresponding numerical IDs according to the model's vocabulary. The resulting `input_tokens` is a sequence of integers that represents our prompt in a format the model can work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d0e26d",
   "metadata": {
    "gather": {
     "logged": 1744965154448
    }
   },
   "outputs": [],
   "source": [
    "input_tokens = tokenizer.encode(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0380ed",
   "metadata": {},
   "source": [
    "## 13. Set Up the Generator\n",
    "\n",
    "Here we configure the text generation process by creating a Generator object with our model:\n",
    "\n",
    "1. First, we create a `GeneratorParams` object associated with our model, which will hold all generation settings\n",
    "\n",
    "2. Then we apply the search options we defined earlier (like maximum length) to these parameters\n",
    "\n",
    "3. Finally, we create the actual `Generator` object that will handle the text generation process\n",
    "\n",
    "This generator will use our model and the specified parameters to generate text based on our input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ab288b",
   "metadata": {
    "gather": {
     "logged": 1744965154554
    }
   },
   "outputs": [],
   "source": [
    "params = og.GeneratorParams(model)\n",
    "params.set_search_options(**search_options)\n",
    "generator = og.Generator(model, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9a4c21",
   "metadata": {},
   "source": [
    "## 14. Activate the LoRA Adapter\n",
    "\n",
    "This important step enables our fine-tuned knowledge by activating the LoRA adapter we loaded earlier. Without this step, the model would run with only its base knowledge.\n",
    "\n",
    "The `set_active_adapter` method connects our LoRA adapter (which we loaded and named \"qa_choice\") to the generator. This adapter contains the specialized knowledge our model learned during fine-tuning to answer multiple-choice questions better.\n",
    "\n",
    "By activating this adapter, we're effectively applying our knowledge distillation improvements to the base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f258a8b",
   "metadata": {
    "gather": {
     "logged": 1744965154650
    }
   },
   "outputs": [],
   "source": [
    "generator.set_active_adapter(adapters, \"qa_choice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b312ac23",
   "metadata": {},
   "source": [
    "## 15. Feed Input Tokens to the Generator\n",
    "\n",
    "Now we provide our tokenized input to the generator. The `append_tokens()` method takes the tokens we created from our prompt and feeds them into the model.\n",
    "\n",
    "At this stage, the model reads and processes the input tokens, but it hasn't started generating a response yet. The model is preparing its internal state based on the input context, which includes the instructions and the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4e0133",
   "metadata": {
    "gather": {
     "logged": 1744965155567
    }
   },
   "outputs": [],
   "source": [
    "generator.append_tokens(input_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bdaf27c",
   "metadata": {},
   "source": [
    "## 16. Generate and Display the Response\n",
    "\n",
    "Finally, we run the text generation process to get our model's answer to the multiple-choice question. This code:\n",
    "\n",
    "1. Uses a `while` loop that continues until the generator declares it's done (either by producing an end token or reaching the maximum length)\n",
    "\n",
    "2. Calls `generate_next_token()` to have the model predict one token at a time\n",
    "\n",
    "3. Gets the most recently generated token with `get_next_tokens()[0]`\n",
    "\n",
    "4. Decodes that token back to text using our tokenizer stream\n",
    "\n",
    "5. Prints each piece of text as it's generated, creating a streaming effect where you see the answer appear gradually\n",
    "\n",
    "If our knowledge distillation and fine-tuning were successful, the model should respond with the letter corresponding to the correct answer choice (in this case, likely \"A\" for \"ignore\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12abc97",
   "metadata": {
    "gather": {
     "logged": 1744965155679
    }
   },
   "outputs": [],
   "source": [
    "while not generator.is_done():\n",
    "            generator.generate_next_token()\n",
    "\n",
    "            new_token = generator.get_next_tokens()[0]\n",
    "            print(tokenizer_stream.decode(new_token), end='', flush=True)"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python38-azureml-pt-tf"
  },
  "kernelspec": {
   "display_name": "Python 3.10 - Pytorch and Tensorflow",
   "language": "python",
   "name": "python38-azureml-pt-tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "microsoft": {
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
