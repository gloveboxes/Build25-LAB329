{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13490c0a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Ensure you login to Azure az login - device\n",
    "\n",
    "Ensure you change the Kernel to Python 3.10 AzureML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddced75",
   "metadata": {
    "gather": {
     "logged": 1744962359523
    }
   },
   "outputs": [],
   "source": [
    "pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebb8ff3",
   "metadata": {
    "gather": {
     "logged": 1744962363192
    }
   },
   "outputs": [],
   "source": [
    "pip install datasets tqdm -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9605ccc",
   "metadata": {
    "gather": {
     "logged": 1744962364326
    }
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from abc import ABC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806f5e03",
   "metadata": {
    "gather": {
     "logged": 1744962364456
    }
   },
   "outputs": [],
   "source": [
    "class InputDataset(ABC):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        (\n",
    "            self.train_data_file_name,\n",
    "            self.test_data_file_name,\n",
    "            self.eval_data_file_name,\n",
    "        ) = (None, None, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34315645",
   "metadata": {
    "gather": {
     "logged": 1744962364607
    }
   },
   "outputs": [],
   "source": [
    "class CQnAHuggingFaceInputDataset(InputDataset):\n",
    "    \"\"\"\n",
    "    Loads the HuggingFace dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def load_hf_dataset(\n",
    "        self,\n",
    "        dataset_name,\n",
    "        train_sample_size=10,\n",
    "        val_sample_size=10,\n",
    "        test_sample_size=10,\n",
    "        train_split_name=\"train\",\n",
    "        val_split_name=\"validation\",\n",
    "        test_split_name=\"test\",\n",
    "    ):\n",
    "        full_dataset = load_dataset(dataset_name)\n",
    "\n",
    "        if val_split_name is not None:\n",
    "            train_data = full_dataset[train_split_name].select(range(train_sample_size))\n",
    "            val_data = full_dataset[val_split_name].select(range(val_sample_size))\n",
    "            test_data = full_dataset[test_split_name].select(range(test_sample_size))\n",
    "        else:\n",
    "            train_val_data = full_dataset[train_split_name].select(\n",
    "                range(train_sample_size + val_sample_size)\n",
    "            )\n",
    "            train_data = train_val_data.select(range(train_sample_size))\n",
    "            val_data = train_val_data.select(\n",
    "                range(train_sample_size, train_sample_size + val_sample_size)\n",
    "            )\n",
    "            test_data = full_dataset[test_split_name].select(range(test_sample_size))\n",
    "\n",
    "        return train_data, val_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a92e682",
   "metadata": {
    "gather": {
     "logged": 1744962367034
    }
   },
   "outputs": [],
   "source": [
    "# We can define train and test sample sizes here. Validation size is kept same as test sample size\n",
    "train_sample_size = 100\n",
    "val_sample_size = 100\n",
    "\n",
    "# Sample notebook using the dataset: https://huggingface.co/datasets/tau/commonsense_qa\n",
    "dataset_name = \"tau/commonsense_qa\"\n",
    "input_dataset = CQnAHuggingFaceInputDataset()\n",
    "\n",
    "# Note: train_split_name and test_split_name can vary by dataset. They are passed as arguments in load_hf_dataset.\n",
    "# If validation_split_name is None, the below function will split the train set to create the specified sized validation set.\n",
    "train, val, _ = input_dataset.load_hf_dataset(\n",
    "    dataset_name=dataset_name,\n",
    "    train_sample_size=train_sample_size,\n",
    "    val_sample_size=val_sample_size,\n",
    "    train_split_name=\"train\",\n",
    "    val_split_name=\"validation\",\n",
    ")\n",
    "\n",
    "print(\"Len of train data sample is \" + str(len(train)))\n",
    "print(\"Len of validation data sample is \" + str(len(val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a458770",
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir -p data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25914972",
   "metadata": {
    "gather": {
     "logged": 1744962367302
    }
   },
   "outputs": [],
   "source": [
    "train_data_path = \"data/train_original_data.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1f3072",
   "metadata": {
    "gather": {
     "logged": 1744962367432
    }
   },
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad051ac1",
   "metadata": {
    "gather": {
     "logged": 1744962370043
    }
   },
   "outputs": [],
   "source": [
    "system_prompt = \"You are a helpful assistant. Your output should only be one of the five choices: 'A', 'B', 'C', 'D', or 'E'.\"\n",
    "user_prompt_template = \"Answer the following multiple-choice question by selecting the correct option.\\n\\nQuestion: {question}\\nAnswer Choices:\\n{answer_choices}\"\n",
    "\n",
    "for row in train:\n",
    "    data = {\"messages\": []}\n",
    "    data[\"messages\"].append(\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": system_prompt,\n",
    "        }\n",
    "    )\n",
    "    question, choices = row[\"question\"], row[\"choices\"]\n",
    "    labels, choice_list = choices[\"label\"], choices[\"text\"]\n",
    "    answer_choices = [\n",
    "        \"({}) {}\".format(labels[i], choice_list[i]) for i in range(len(labels))\n",
    "    ]\n",
    "    answer_choices = \"\\n\".join(answer_choices)\n",
    "    data[\"messages\"].append(\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": user_prompt_template.format(\n",
    "                question=question, answer_choices=answer_choices\n",
    "            ),\n",
    "        }\n",
    "    )\n",
    "    with open(train_data_path, \"a\") as f:\n",
    "        f.write(json.dumps(data) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfde7fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "print(\"Loading environment variables from .env file...\")\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8fc57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Azure AI Foundry credentials from environment variables\n",
    "teacher_model_name = os.getenv('TEACHER_MODEL_NAME')\n",
    "teacher_model_endpoint_url = os.getenv('TEACHER_MODEL_ENDPOINT')\n",
    "teacher_model_api_key = os.getenv('TEACHER_MODEL_KEY')\n",
    "\n",
    "# Print values for debugging (remove in production)\n",
    "print(f\"Teacher Model Name: {teacher_model_name}\")\n",
    "print(f\"Teacher Model Endpoint: {teacher_model_endpoint_url}\")\n",
    "# Don't print the API key for security reasons\n",
    "print(f\"Teacher Model API Key loaded: {'Yes' if teacher_model_api_key else 'No'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5052dde2",
   "metadata": {
    "gather": {
     "logged": 1744962373633
    }
   },
   "outputs": [],
   "source": [
    "pip install azure-ai-inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fb1cbf",
   "metadata": {
    "gather": {
     "logged": 1744962373771
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.ai.inference.models import SystemMessage, UserMessage\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f12ae7",
   "metadata": {
    "gather": {
     "logged": 1744962373924
    }
   },
   "outputs": [],
   "source": [
    "def process_question(question_data):\n",
    "    try:\n",
    "        messages = []\n",
    "        for msg in question_data[\"messages\"]:\n",
    "            if msg[\"role\"] == \"system\":\n",
    "                messages.append(SystemMessage(content=msg[\"content\"]))\n",
    "            elif msg[\"role\"] == \"user\":\n",
    "                messages.append(UserMessage(content=msg[\"content\"]))\n",
    "\n",
    "        response = client.complete(\n",
    "            messages=messages,\n",
    "            model=model_name,\n",
    "            max_tokens=4000  # Reduced since we just need short answers like A, B, C, D, or E\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"question\": question_data[\"messages\"][1][\"content\"],\n",
    "            \"response\": response.choices[0].message.content,\n",
    "            \"full_response\": response\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"question\": question_data[\"messages\"][1][\"content\"] if len(question_data[\"messages\"]) > 1 else \"Error\",\n",
    "            \"response\": f\"Error: {str(e)}\",\n",
    "            \"full_response\": None\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448b00cf",
   "metadata": {
    "gather": {
     "logged": 1744962374043
    }
   },
   "outputs": [],
   "source": [
    "endpoint = teacher_model_endpoint_url\n",
    "model_name = teacher_model_name\n",
    "key = teacher_model_api_key\n",
    "client = ChatCompletionsClient(endpoint=endpoint, credential=AzureKeyCredential(key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285ab8b3",
   "metadata": {
    "gather": {
     "logged": 1744962604578
    }
   },
   "outputs": [],
   "source": [
    "# Read the JSONL file and process each question\n",
    "results = []\n",
    "\n",
    "# First, count the number of lines in the file for the progress bar\n",
    "with open(train_data_path, 'r', encoding='utf-8') as file:\n",
    "    total_lines = sum(1 for _ in file if _.strip())\n",
    "\n",
    "print(f\"Processing {total_lines} questions from {train_data_path}\")\n",
    "\n",
    "with open(train_data_path, 'r', encoding='utf-8') as file:\n",
    "    # Initialize tqdm progress bar\n",
    "    progress_bar = tqdm(total=total_lines, desc=\"Processing questions\", unit=\"question\")\n",
    "    \n",
    "    for i, line in enumerate(file):\n",
    "        if line.strip():  # Skip empty lines\n",
    "            try:\n",
    "                question_data = json.loads(line)\n",
    "                result = process_question(question_data)\n",
    "                results.append(result)\n",
    "                \n",
    "                # Update progress bar description with latest result\n",
    "                progress_bar.set_description(f\"Latest answer: {result['response']}\")\n",
    "                progress_bar.update(1)\n",
    "                \n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error parsing line {i+1}: {str(e)}\")\n",
    "                progress_bar.update(1)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing line {i+1}: {str(e)}\")\n",
    "                progress_bar.update(1)\n",
    "    \n",
    "    progress_bar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6de3fb",
   "metadata": {
    "gather": {
     "logged": 1744962804797
    }
   },
   "outputs": [],
   "source": [
    "output_file_path = \"./data/train_data.jsonl\"\n",
    "\n",
    "# Initialize tqdm progress bar for writing results\n",
    "print(f\"Writing {len(results)} processed questions to {output_file_path}\")\n",
    "with open(output_file_path, 'w', encoding='utf-8') as f:\n",
    "    for result in tqdm(results, desc=\"Writing results\", unit=\"record\"):\n",
    "        # Extract just the question content (removing the instruction part)\n",
    "        question_text = result[\"question\"]\n",
    "        # if \"Question: \" in question_text:\n",
    "        #     question_text = question_text.split(\"Question: \")[1].split(\"\\nAnswer Choices:\")[0]\n",
    "\n",
    "        # Create the simplified output format\n",
    "        output_line = {\n",
    "            \"Question\": question_text,\n",
    "            \"Answer\": result[\"response\"]\n",
    "        }\n",
    "\n",
    "        # Write as JSONL (one JSON object per line)\n",
    "        f.write(json.dumps(output_line, ensure_ascii=False) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de00c5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the distillation results\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Count occurrences of each answer\n",
    "answer_counts = {}\n",
    "other_responses = []\n",
    "\n",
    "for result in results:\n",
    "    answer = result['response'].strip()\n",
    "    \n",
    "    # Try to extract just the letter if there's additional text\n",
    "    # This regex matches: A, A., A), (A), (A) text\n",
    "    match = re.search(r'(?:^|\\(|\\s)([A-E])(?:\\)|\\.|\\s|$)', answer, re.IGNORECASE)\n",
    "    if match:\n",
    "        answer = match.group(1).upper()  # Extract just the letter and convert to uppercase\n",
    "        answer_counts[answer] = answer_counts.get(answer, 0) + 1\n",
    "    else:\n",
    "        # For non-standard answers\n",
    "        answer_counts['Other'] = answer_counts.get('Other', 0) + 1\n",
    "        # Store problematic responses for analysis\n",
    "        other_responses.append(answer)\n",
    "\n",
    "# Create a DataFrame for visualization\n",
    "df = pd.DataFrame(list(answer_counts.items()), columns=['Answer', 'Count'])\n",
    "df = df.sort_values('Count', ascending=False)\n",
    "\n",
    "# Create a bar chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(df['Answer'], df['Count'], color='skyblue')\n",
    "plt.title('Distribution of Answers in Distilled Dataset')\n",
    "plt.xlabel('Answer Choice')\n",
    "plt.ylabel('Count')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Add count labels on top of each bar\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
    "            f'{height}', ha='center', va='bottom')\n",
    "\n",
    "# Calculate and display success rate (assuming valid answers are A-E)\n",
    "total_questions = sum(answer_counts.values())\n",
    "valid_answers = sum(answer_counts.get(ans, 0) for ans in ['A', 'B', 'C', 'D', 'E'])\n",
    "success_rate = (valid_answers / total_questions) * 100 if total_questions > 0 else 0\n",
    "\n",
    "plt.figtext(0.5, 0.01, f'Success Rate: {success_rate:.1f}% ({valid_answers}/{total_questions} questions with valid answers)', \n",
    "           ha='center', fontsize=12)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.05, 1, 0.95])\n",
    "plt.show()\n",
    "\n",
    "# Display sample of 'Other' responses if they exist\n",
    "if other_responses:\n",
    "    print(f\"\\nSamples of 'Other' responses (showing up to 10):\")\n",
    "    for i, resp in enumerate(other_responses[:10]):\n",
    "        print(f\"  {i+1}. '{resp}'\")\n",
    "    \n",
    "    # Analyze if there are patterns in the 'Other' responses\n",
    "    lowercase_letters = sum(1 for r in other_responses if r.lower() in ['a', 'b', 'c', 'd', 'e'])\n",
    "    has_period = sum(1 for r in other_responses if re.search(r'[A-Ea-e]\\.', r))\n",
    "    has_explanation = sum(1 for r in other_responses if len(r) > 5)  # Simple heuristic for explanations\n",
    "    \n",
    "    print(f\"\\nAnalysis of {len(other_responses)} 'Other' responses:\")\n",
    "    print(f\"  - Lowercase letters (a-e): {lowercase_letters} ({lowercase_letters/len(other_responses)*100:.1f}%)\")\n",
    "    print(f\"  - Answers with periods (A.): {has_period} ({has_period/len(other_responses)*100:.1f}%)\")\n",
    "    print(f\"  - Likely explanations (>5 chars): {has_explanation} ({has_explanation/len(other_responses)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be286dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze potential model bias in answer distribution\n",
    "\n",
    "# Expected distribution (ideally uniform for multiple choice)\n",
    "expected_prob = 0.2  # 20% chance for each of A,B,C,D,E in a uniform distribution\n",
    "expected_counts = {letter: total_questions * expected_prob for letter in ['A', 'B', 'C', 'D', 'E']}\n",
    "\n",
    "# Create a DataFrame for comparing actual vs expected\n",
    "comparison_data = []\n",
    "for letter in ['A', 'B', 'C', 'D', 'E']:\n",
    "    actual = answer_counts.get(letter, 0)\n",
    "    expected = expected_counts[letter]\n",
    "    difference = actual - expected\n",
    "    percent_diff = (difference / expected) * 100 if expected > 0 else 0\n",
    "    comparison_data.append({\n",
    "        'Answer': letter,\n",
    "        'Actual Count': actual,\n",
    "        'Expected Count': expected,\n",
    "        'Difference': difference,\n",
    "        'Percent Difference': percent_diff\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(\"\\nAnalysis of potential answer bias:\")\n",
    "display(comparison_df)\n",
    "\n",
    "# Create a visual comparison\n",
    "plt.figure(figsize=(12, 6))\n",
    "x = range(len(comparison_df))\n",
    "width = 0.35\n",
    "\n",
    "plt.bar([i - width/2 for i in x], comparison_df['Actual Count'], width, label='Actual', color='skyblue')\n",
    "plt.bar([i + width/2 for i in x], comparison_df['Expected Count'], width, label='Expected', color='lightgreen')\n",
    "\n",
    "plt.xlabel('Answer Choice')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Actual vs Expected Answer Distribution')\n",
    "plt.xticks(x, comparison_df['Answer'])\n",
    "plt.legend()\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   },
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
