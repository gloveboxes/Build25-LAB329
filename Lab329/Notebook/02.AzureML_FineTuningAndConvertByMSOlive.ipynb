{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bf944c2",
   "metadata": {},
   "source": [
    "# Fine-tuning and Optimizing a Student Model\n",
    "\n",
    "This notebook demonstrates how to fine-tune a small \"student\" model using the training data we generated from our \"teacher\" model in the previous notebook. We'll also optimize the model for efficient deployment.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "- How to use Microsoft Olive to fine-tune the Phi-4-mini model\n",
    "- How to apply Low-Rank Adaptation (LoRA) for efficient fine-tuning\n",
    "- How to convert a model to ONNX format for optimization\n",
    "- How to apply quantization to reduce model size\n",
    "- How to prepare the model for deployment on resource-constrained environments\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Completed the previous notebook (`01.AzureML_Distillation.ipynb`)\n",
    "- Generated training data in `data/train_data.jsonl`\n",
    "- Access to Azure ML with the Phi-4-mini model in the registry\n",
    "- Python environment with necessary libraries (which we'll install)\n",
    "\n",
    "## Setup Instructions\n",
    "\n",
    "1. **Azure Authentication**: Ensure you're logged in to Azure using `az login --use-device-code` in a terminal\n",
    "2. **Kernel Selection**: Change the Jupyter kernel to **\"Python 3.10 PyTorch and Tensorflow\"** using the selector in the top right\n",
    "3. **Environment File**: Ensure your `local.env` file exists with proper credentials"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f080269",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Initial Setup\n",
    "\n",
    "Before we begin, make sure you've completed these steps:\n",
    "\n",
    "1. **Azure Login**: Run `az login --use-device-code` in a terminal to authenticate with Azure\n",
    "\n",
    "2. **Kernel Selection**: Select the \"Python 3.10 PyTorch and Tensorflow\" kernel from the dropdown in the top-right corner. This kernel has most of the dependencies we need pre-installed.\n",
    "\n",
    "3. **Check Environment**: Ensure your `local.env` file is in the same directory as this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8487c5dc",
   "metadata": {},
   "source": [
    "## 1. Install Required Packages\n",
    "\n",
    "First, we'll install the packages needed for this notebook. The `dotenv-azd` package allows us to securely load environment variables from Azure Developer CLI (AZD) environments and/or local .env files. This is how we'll access our Azure credentials without hardcoding sensitive information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33b08d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install dotenv-azd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac72d0e",
   "metadata": {},
   "source": [
    "## 2. Load Environment Variables\n",
    "\n",
    "This cell loads our environment variables which contain the credentials and configuration needed for Azure ML. We use two approaches:\n",
    "\n",
    "1. **Azure Developer CLI (AZD) Environment**: First, try to load variables from an AZD environment if available\n",
    "\n",
    "2. **Local .env File**: As a fallback, load variables from a local.env file\n",
    "\n",
    "This ensures our credentials are kept secure and separate from our code, following security best practices for handling sensitive information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d7a855",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv_azd import load_azd_env\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from current AZD environment if available\n",
    "load_azd_env(quiet=True)\n",
    "\n",
    "# Load environment variables from local.env file if it exists\n",
    "load_dotenv(dotenv_path=\"local.env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5d712c",
   "metadata": {},
   "source": [
    "## 3. Install Authentication Packages\n",
    "\n",
    "Here we install the packages needed to authenticate with Azure services:\n",
    "\n",
    "- **python-dotenv**: For loading environment variables from .env files\n",
    "- **azure-identity**: Provides credential classes for authentication with Azure\n",
    "- **azure-ai-ml**: The Azure ML SDK for working with Azure Machine Learning\n",
    "\n",
    "The `-U` flag ensures we get the latest versions of these packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d617742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages for authentication\n",
    "! pip install python-dotenv azure-identity azure-ai-ml -U"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fdb4c93",
   "metadata": {},
   "source": [
    "## 4. Install Azure AI Inference SDK\n",
    "\n",
    "We're installing the `azure-ai-inference` package, which is Microsoft's official SDK for interacting with Azure AI models. This library will allow us to:\n",
    "\n",
    "- Connect to Azure AI services\n",
    "- Send prompts to foundation models\n",
    "- Process model responses\n",
    "- Configure model parameters\n",
    "\n",
    "We'll use this to verify our fine-tuned model after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96ef9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install azure-ai-inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43939308",
   "metadata": {},
   "source": [
    "## 5. Install Microsoft Olive\n",
    "\n",
    "Now we install Microsoft Olive, an open-source model optimization toolkit that will be the main tool for our fine-tuning and optimization process. The `[auto-opt]` option includes additional dependencies for automatic optimization.\n",
    "\n",
    "Olive provides:\n",
    "- Model fine-tuning capabilities\n",
    "- ONNX conversion tools\n",
    "- Quantization for model compression\n",
    "- Performance optimization for various hardware targets\n",
    "\n",
    "This powerful tool will help us efficiently fine-tune our model and prepare it for deployment on resource-constrained devices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f3ef60",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install olive-ai[auto-opt] -U"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77fc5c13",
   "metadata": {},
   "source": [
    "## 6. Verify Olive Installation\n",
    "\n",
    "We'll now check the installed version of Olive to ensure it installed correctly. This command shows:\n",
    "- The package name\n",
    "- The installed version number\n",
    "- Where the package is installed\n",
    "- The package's dependencies\n",
    "\n",
    "Confirming the version is important as different versions of Olive may have different features or requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08355c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip show olive-ai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6a3bdf",
   "metadata": {},
   "source": [
    "## 7. Install ONNX Runtime GenAI\n",
    "\n",
    "Next, we install ONNX Runtime GenAI, a specialized version of ONNX Runtime designed specifically for generative AI models. This package will allow us to:\n",
    "\n",
    "- Run our optimized model efficiently\n",
    "- Leverage specialized optimizations for transformer models\n",
    "- Access adapter-based fine-tuning capabilities\n",
    "\n",
    "We're installing version 0.7.1 with the `--pre` flag because it's a pre-release version with features we need for our work. Later notebooks will use this to run inference with our optimized model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69907a09",
   "metadata": {
    "gather": {
     "logged": 1744963274226
    }
   },
   "outputs": [],
   "source": [
    "pip install onnxruntime-genai==0.7.1 --pre"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97d1dc8",
   "metadata": {},
   "source": [
    "## 8. Package Management\n",
    "\n",
    "The next few cells handle package management to avoid conflicts. We're:\n",
    "\n",
    "1. **Uninstalling onnxruntime-gpu** to avoid conflicts with the regular onnxruntime package\n",
    "2. **Installing regular onnxruntime** for CPU-based inference\n",
    "3. **Installing additional dependencies** including:\n",
    "   - bitsandbytes: For efficient quantization\n",
    "   - transformers: For working with transformer models\n",
    "   - peft: For parameter-efficient fine-tuning (LoRA)\n",
    "   - accelerate: For optimized training\n",
    "\n",
    "These packages will ensure our environment is properly set up for fine-tuning and optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba394391",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip uninstall onnxruntime-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cdfae44",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install onnxruntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c74e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b0305c",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install transformers==4.49.0 -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd93b0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install azure-ai-ml -U  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fb7020",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install marshmallow==3.23.2 -U   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b21c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8207d88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install numpy==1.23.5 -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165e19c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install --upgrade accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5eed001",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip show accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692ca04b-6242-42c7-8dff-3d5f8b1122b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install peft"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6ab990",
   "metadata": {},
   "source": [
    "## 9. Initialize Azure Authentication\n",
    "\n",
    "Here we set up authentication with Azure using the AzureCliCredential. This credential uses your existing Azure CLI login to authenticate requests to Azure services.\n",
    "\n",
    "1. We import the `AzureCliCredential` class from the azure-identity package\n",
    "2. We create an instance of this credential\n",
    "\n",
    "This credential will be used when we access models from the Azure ML registry during our fine-tuning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854bf01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.identity import AzureCliCredential\n",
    "credential = AzureCliCredential()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378a60ad",
   "metadata": {},
   "source": [
    "## 10. Fine-tune with Low-Rank Adaptation (LoRA)\n",
    "\n",
    "This is the core command that fine-tunes our student model. We're using Microsoft Olive's fine-tuning capabilities with LoRA (Low-Rank Adaptation), a parameter-efficient fine-tuning method. Here's what each parameter does:\n",
    "\n",
    "- **`--method lora`**: Use Low-Rank Adaptation, which adds small trainable matrices to key layers instead of updating all weights\n",
    "\n",
    "- **`--model_name_or_path`**: The base model to fine-tune (Phi-4-mini-instruct from Azure ML registry)\n",
    "\n",
    "- **`--trust_remote_code`**: Allow execution of code from the remote model repository\n",
    "\n",
    "- **`--data_name json`**: The format of our training data (JSON)\n",
    "\n",
    "- **`--data_files`**: Path to our training data generated from the teacher model\n",
    "\n",
    "- **`--text_template`**: Template for formatting inputs and outputs during training\n",
    "\n",
    "- **`--max_steps 100`**: Only train for 100 steps (for speed, in production you'd use more)\n",
    "\n",
    "- **`--output_path`**: Where to save the fine-tuned model and adapter\n",
    "\n",
    "- **`--target_modules`**: Which layers to apply LoRA to (attention and feed-forward layers)\n",
    "\n",
    "- **`--log_level 1`**: Set verbosity of logging\n",
    "\n",
    "This process will take several minutes to complete. It creates a LoRA adapter that captures the knowledge our model learned from the teacher without modifying the base model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22241278",
   "metadata": {
    "gather": {
     "logged": 1744964309865
    }
   },
   "outputs": [],
   "source": [
    "! olive finetune \\\n",
    "    --method lora \\\n",
    "    --model_name_or_path  azureml://registries/azureml/models/Phi-4-mini-instruct/versions/1 \\\n",
    "    --trust_remote_code \\\n",
    "    --data_name json \\\n",
    "    --data_files ./data/train_data.jsonl \\\n",
    "    --text_template \"<|user|>{Question}<|end|><|assistant|>{Answer}<|end|>\" \\\n",
    "    --max_steps 100 \\\n",
    "    --output_path models/phi-4-mini/ft \\\n",
    "    --target_modules \"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\" \\\n",
    "    --log_level 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e50cfd3",
   "metadata": {},
   "source": [
    "## 11. Reinstall ONNX Runtime GenAI\n",
    "\n",
    "Here we reinstall ONNX Runtime GenAI to ensure we have the correct version after all our package management. This is a precautionary step to make sure we have the version (0.7.1) needed for our model optimization in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03d6cc0-db61-4613-bc89-42fa13c06440",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install onnxruntime-genai==0.7.1 --pre"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1521bfae",
   "metadata": {},
   "source": [
    "## 12. Optimize and Quantize the Model\n",
    "\n",
    "This command uses Microsoft Olive's auto-optimization capabilities to convert our fine-tuned model to ONNX format and apply int4 quantization. Here's what each parameter does:\n",
    "\n",
    "- **`--model_name_or_path`**: The base model from Azure ML registry\n",
    "\n",
    "- **`--adapter_path`**: Path to our LoRA adapter created in the previous step\n",
    "\n",
    "- **`--device cpu`**: Target CPU for optimization (you could also use cuda for GPU)\n",
    "\n",
    "- **`--provider CPUExecutionProvider`**: Use the CPU execution provider for ONNX Runtime\n",
    "\n",
    "- **`--use_model_builder`**: Use Olive's model builder for optimized conversion\n",
    "\n",
    "- **`--precision int4`**: Apply int4 quantization, which reduces model size by up to 75% compared to FP16\n",
    "\n",
    "- **`--output_path`**: Where to save the optimized model\n",
    "\n",
    "- **`--log_level 1`**: Set verbosity of logging\n",
    "\n",
    "The optimization process:\n",
    "1. Merges the base model with our LoRA adapter\n",
    "2. Converts to ONNX format, which is more efficient for inference\n",
    "3. Applies int4 quantization to dramatically reduce model size\n",
    "4. Optimizes the model for CPU inference\n",
    "\n",
    "This process will take several minutes to complete. The result will be a much smaller, more efficient model that can run on devices with limited resources while maintaining most of the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ff7789",
   "metadata": {
    "gather": {
     "logged": 1744965032561
    }
   },
   "outputs": [],
   "source": [
    "! olive auto-opt \\\n",
    "    --model_name_or_path  azureml://registries/azureml/models/Phi-4-mini-instruct/versions/1 \\\n",
    "    # model_name_or_path  models/phi-4-mini/ft/model/data/model \\\n",
    "    --adapter_path models/phi-4-mini/ft/adapter \\\n",
    "    --device cpu \\\n",
    "    --provider CPUExecutionProvider \\\n",
    "    --use_model_builder \\\n",
    "    --precision int4 \\\n",
    "    --output_path models/phi-4-mini/onnx \\\n",
    "    --log_level 1"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python38-azureml-pt-tf"
  },
  "kernelspec": {
   "display_name": "Python 3.8 - Pytorch and Tensorflow",
   "language": "python",
   "name": "python38-azureml-pt-tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "microsoft": {
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
